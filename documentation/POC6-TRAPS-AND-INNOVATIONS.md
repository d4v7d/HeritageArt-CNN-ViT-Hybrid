# POC-6: Implementation Traps & Proposed Innovations

**Document Created**: October 26, 2025  
**Purpose**: Permanent record of critical issues and innovative solutions for POC-6 multiclass segmentation + domain generalization.  
**Context**: After POC-5 (binary segmentation, MaxViT winner 71.64% mIoU), scaling to 16-class multiclass + DG evaluation.

---

## ðŸš¨ IMPLEMENTATION TRAPS (6 Critical Issues)

### Trap 1: Dataset Size Mismatch
**Issue**: POC-6 plan assumes ~11,000 annotations, but HuggingFace dataset viewer shows **"Estimated number of rows: 445"**.

**Impact**:
- 445 samples Ã· 10 materials = ~44 samples/material (below minimum threshold of 100)
- LOMO cross-validation may have insufficient training data
- Class imbalance worse than expected (rare damage types <10 samples)

**Mitigation**:
- **CRITICAL**: Download full ARTeFACT dataset first to verify actual size
- If <1000 samples: merge sparse materials (e.g., Wood+Canvasâ†’Traditional, Glass+Ceramicâ†’Rigid) from 10â†’5 materials
- If <100 samples for rare classes: hierarchical classification (binary â†’ coarse â†’ fine)
- Consider data augmentation multiplier (10x-20x for rare classes via CutMix/MixUp)

**Command to verify**:
```bash
# Download and count actual samples
huggingface-cli download danielaivanova/damaged-media --repo-type dataset
python -c "import datasets; ds = datasets.load_dataset('danielaivanova/damaged-media'); print(len(ds['train']))"
```

---

### Trap 2: Class Imbalance (Severe in Heritage Domain)
**Issue**: ARTeFACT has 16 classes (Clean + 15 damage types), but class distribution is heavily skewed.

**Expected Distribution** (from dataset card examples):
- **Frequent**: Dirt spot (30%+), Discolouration (25%), Material loss (20%)
- **Moderate**: Cracks (10%), Peel (8%), Stains (7%)
- **Rare**: Lightleak (<2%), Burn marks (<1%), Hairs (<1%)

**Impact**:
- Naive training will learn to predict only frequent classes
- Rare class IoU â‰ˆ 0% (model ignores them completely)
- Misleading overall mIoU (dominated by frequent classes)

**Mitigation**:
- **Class weighting**: Inverse frequency weighting (sqrt or log dampening to avoid extremes)
  ```python
  weights = torch.tensor([1.0, 3.5, 2.8, 4.2, 5.1, 3.9, ..., 25.0, 30.0])  # Per class
  criterion = DiceFocalLoss(class_weights=weights)
  ```
- **Hierarchical Multi-Task Learning** (Innovation #1): 3 parallel heads force model to learn rare classes
- **Oversampling rare classes**: Repeat rare class samples 5x-10x in dataloader
- **Damage-Aware Attention** (Innovation #4): Prototype learning for rare damage patterns

---

### Trap 3: LOMO Sample Sufficiency
**Issue**: Leave-One-Material-Out requires each material to have sufficient samples for validation.

**Math**:
- 445 total samples Ã· 10 materials = ~44 samples/material
- Minimum recommended: 100 samples for reliable validation
- **Result**: Cannot perform robust LOMO with 10-way split

**Impact**:
- High variance in DG metrics (different materials have 20-80 samples each)
- Unreliable conclusions about which architecture generalizes best
- Overfitting to small validation sets

**Mitigation**:
- **Material grouping**: Merge similar materials to create balanced groups
  - Group 1: **Parchment + Paper** (flexible organic)
  - Group 2: **Canvas + Textile** (woven)
  - Group 3: **Wood + Film emulsion** (layered)
  - Group 4: **Glass + Ceramic + Tesserae** (rigid inorganic)
  - Group 5: **Lime plaster** (architectural)
  - Result: 10 materials â†’ 5 groups (each ~90 samples)
- **LOContent instead**: Only 4 content types (artistic, photographic, line art, geometric) = ~111 samples each
- **K-Fold stratified**: 5-fold CV stratified by material (more robust than LOMO)

---

### Trap 4: Training Time Underestimation
**Issue**: POC-6 plan estimates "6-8h per model" but multiclass is 3x-5x slower than binary.

**Reality Check**:
- POC-5 binary (2 classes, 50 samples): 60 epochs = 25 minutes/epoch Ã— 60 = **25 hours**
- POC-6 multiclass (16 classes, 445 samples): 
  - Dataset 9x larger: 25 min Ã— 9 = 225 min/epoch
  - More classes (slower loss computation): 225 Ã— 1.3 = 292 min/epoch
  - 100 epochs planned: 292 Ã— 100 = **29,200 min â‰ˆ 487 hours â‰ˆ 20 days GPU time**
- **Per model**: ~487h Ã— 3 models = **1,461 GPU hours** just for Phase 2

**Impact**:
- Original plan (42 DG runs Ã— 487h = 20,454 GPU hours) is **computationally infeasible**
- Even multiclass baseline (3 models Ã— 487h = 1,461h) takes 61 days continuous

**Mitigation**:
- **Reduce epochs**: 100 â†’ 60 epochs (POC-5 showed 60 sufficient)
- **Aggressive early stopping**: patience=10 (vs 15), min_delta=0.001
- **Mixed precision**: AMP (Automatic Mixed Precision) saves 30-40% time
- **Smaller input resolution**: 512Ã—512 â†’ 384Ã—384 (saves 44% compute)
- **MAML Meta-Learning** (Innovation #3): Replace 42 training runs with 1 meta-learning run
- **Progressive Curriculum** (Innovation #5): Binary (20 ep) â†’ Coarse (20 ep) â†’ Fine (60 ep) = 100 epochs but warmer start

**Revised Estimate** (with mitigations):
- 60 epochs, AMP, 384Ã—384 resolution: **487h â†’ 140h per model**
- 3 models: 140 Ã— 3 = **420 GPU hours â‰ˆ 17.5 days**

---

### Trap 5: 42 DG Training Runs (Computational Explosion)
**Issue**: POC-6 Phase 5 plans 30 LOMO runs + 12 LOContent runs = **42 independent training runs**.

**Math**:
- 42 runs Ã— 60 epochs Ã— 292 min/epoch = **734,400 min = 12,240 hours = 510 days GPU time**
- Even with mitigations (AMP, 384px): 42 Ã— 140h = **5,880 hours = 245 days**

**Impact**:
- Completely infeasible on single GPU (would take 8+ months)
- Requires compute cluster ($10k+ cloud costs)
- Delays research by 6-12 months

**Mitigation**:
- **MAML Meta-Learning** (Innovation #3): 
  - **1 training run** learns optimal initialization for fast adaptation
  - At test time: fine-tune on support set (5-10 samples) for 10 steps
  - Result: 42 runs â†’ **1 meta-training run** + 42 fast adaptations (5 min each)
  - Total: 140h (meta-train) + 3.5h (42 adaptations) = **143.5h vs 5,880h** (41x speedup!)
- **Sample 5 materials**: Instead of 10 LOMO runs, select 5 diverse materials (60% reduction)
- **LOContent only**: Focus on 4 content types (easier to interpret than 10 materials)
- **Domain-Invariant Features**: Train once with CORAL/IRM losses (no separate runs needed)

---

### Trap 6: Multiclass Difficulty Underestimated
**Issue**: POC-6 plan expects "50-55% mIoU" for multiclass, but this is **too optimistic** without innovations.

**Reality Check**:
- POC-5 binary: MaxViT 71.64% mIoU (2 classes, balanced dataset)
- Literature baselines (ADE20K, COCO-Stuff):
  - UPerNet-ResNet50 on ADE20K (150 classes): **42.6% mIoU**
  - UPerNet-Swin-T on ADE20K: **44.5% mIoU**
- ARTeFACT challenges:
  - Severe class imbalance (30:1 ratio)
  - Small dataset (445 vs ADE20K 20k)
  - Fine-grained damage types (Cracks vs Structural cracks vs Material loss cracks)
  - Heritage domain shift from ImageNet pretraining

**Expected Naive Performance**:
- ConvNeXt-Tiny: **35-40% mIoU** (struggles with rare classes)
- Swin-Tiny: **38-42% mIoU** (better global context)
- MaxViT-Tiny: **40-45% mIoU** (hybrid advantage)

**Impact**:
- Disappointing results may lead to questioning research value
- Rare class IoU near 0% (model learns only frequent classes)
- Hard to answer RQ1 conclusively if all models perform poorly

**Mitigation**:
- **Hierarchical Multi-Task Learning** (Innovation #1): +5-8% mIoU boost
- **Self-Supervised MAE Pretraining** (Innovation #2): +10-15% mIoU (domain-specific features)
- **Damage-Aware Attention** (Innovation #4): +3-5% mIoU on rare classes
- **Progressive Curriculum** (Innovation #5): +4-6% mIoU (warm start from binary)
- **Combined innovations**: Target **50-55% mIoU** becomes achievable

---

## ðŸ’¡ PROPOSED INNOVATIONS (6 Game-Changers)

### Innovation 1: Hierarchical Multi-Task Learning
**Motivation**: Rare classes are hard to learn directly. Guide learning with easier auxiliary tasks.

**Approach**: 3 parallel prediction heads at different granularities:

```python
class HierarchicalUPerNet(nn.Module):
    def __init__(self, encoder, num_classes=16):
        super().__init__()
        self.encoder = encoder
        self.upernet_neck = UPerNetNeck(...)  # Shared PPM + FPN
        
        # 3 parallel heads (all use same features from neck)
        self.head_binary = nn.Conv2d(channels, 2, 1)      # Clean vs Damage
        self.head_coarse = nn.Conv2d(channels, 4, 1)      # 4 damage groups
        self.head_fine = nn.Conv2d(channels, 16, 1)       # 16 fine classes
        
    def forward(self, x):
        features = self.encoder(x)
        neck_out = self.upernet_neck(features)
        
        return {
            'binary': self.head_binary(neck_out),    # Auxiliary task 1
            'coarse': self.head_coarse(neck_out),    # Auxiliary task 2
            'fine': self.head_fine(neck_out)         # Main task
        }

# Loss function
loss = (
    1.0 * dice_focal_loss(pred['fine'], target_fine) +      # Main task
    0.3 * dice_focal_loss(pred['coarse'], target_coarse) +  # Auxiliary 1
    0.2 * dice_focal_loss(pred['binary'], target_binary)    # Auxiliary 2
)
```

**Class Grouping** (coarse 4 groups):
1. **Structural Damage**: Cracks, Material loss, Peel, Structural defects
2. **Surface Contamination**: Dirt spots, Stains, Hairs, Dust spots
3. **Color Alterations**: Discolouration, Burn marks, Fading
4. **Optical Artifacts**: Scratches, Lightleak, Blur

**Expected Benefits**:
- Easier learning (binary 71% â†’ coarse 60% â†’ fine 50% IoU cascade)
- Rare classes benefit from coarse-level guidance
- **+5-8% mIoU** improvement over single-head baseline
- Interpretable: can analyze where model fails (coarse vs fine-grained mistakes)

**Implementation Effort**: ~150 lines code, +2 days work

---

### Innovation 2: Self-Supervised MAE Pretraining (Domain Adaptation)
**Motivation**: ImageNet pretraining learns cats/dogs/cars, not cracks/stains/heritage textures.

**Approach**: Masked Autoencoder (MAE) pretraining on ARTeFACT unlabeled images.

```python
# Phase 0.5: MAE Pretraining (before multiclass training)
class MAEPretrainer:
    def __init__(self, encoder):
        self.encoder = encoder  # ViT or Swin or MaxViT
        self.decoder = LightweightDecoder()  # Reconstruct masked patches
        
    def pretrain(self, unlabeled_images, epochs=50):
        for img in unlabeled_images:
            # Mask 75% of patches randomly
            masked_img, mask, target = mask_image(img, mask_ratio=0.75)
            
            # Encode visible patches
            features = self.encoder(masked_img)
            
            # Decode and reconstruct
            reconstruction = self.decoder(features, mask)
            
            # Loss: MSE between original and reconstructed
            loss = F.mse_loss(reconstruction, target)
            loss.backward()
        
        return self.encoder  # Return pretrained encoder

# Usage
encoder_pretrained = MAEPretrainer(encoder).pretrain(artefact_unlabeled, epochs=50)
model = HierarchicalUPerNet(encoder_pretrained, num_classes=16)
```

**Dataset for Pretraining**:
- Use all 445 ARTeFACT images (no labels needed)
- Can augment to 4,450 synthetic images (10x via MixUp/CutMix/Style transfer)
- Learn: crack patterns, heritage textures, damage morphology, lighting conditions

**Expected Benefits**:
- **+10-15% mIoU** (domain-specific features vs ImageNet init)
- Better generalization (learns invariances specific to heritage art)
- Rare class boost (encoder learns to reconstruct rare damage patterns)
- State-of-the-art: MAE on medical images: +12% dice score (similar small-dataset domain)

**Implementation Effort**: ~300 lines code, +10 hours training, +3 days work

**Trade-off**: +50 epochs pretraining (10h GPU) before main training (worth it for +15% mIoU)

---

### Innovation 3: MAML Meta-Learning for Efficient DG
**Motivation**: 42 LOMO/LOContent training runs = 5,880 GPU hours (infeasible). Learn to adapt fast.

**Approach**: Model-Agnostic Meta-Learning (MAML) learns initialization optimized for few-shot adaptation.

```python
# Replace 42 training runs with 1 meta-learning run
class MAMLMetaLearner:
    def __init__(self, model):
        self.model = model
        self.meta_lr = 1e-4  # Outer loop learning rate
        self.inner_lr = 1e-3  # Inner loop (adaptation) learning rate
        
    def meta_train(self, tasks):
        """
        tasks = [
            {'support': (X_train_material1, y_train_material1), 
             'query': (X_val_material1, y_val_material1)},
            {'support': (X_train_material2, y_train_material2), 
             'query': (X_val_material2, y_val_material2)},
            ...  # 10 materials + 4 contents = 14 tasks
        ]
        """
        for meta_epoch in range(100):
            meta_loss = 0
            
            for task in sample_tasks(tasks, batch_size=4):  # 4 tasks per meta-batch
                # Inner loop: adapt to task with few examples
                model_copy = clone_model(self.model)
                for step in range(5):  # 5 gradient steps
                    loss = compute_loss(model_copy, task['support'])
                    model_copy = update(model_copy, loss, lr=self.inner_lr)
                
                # Outer loop: evaluate adapted model on query set
                query_loss = compute_loss(model_copy, task['query'])
                meta_loss += query_loss
            
            # Meta-update: update initialization to minimize query loss
            self.model = update(self.model, meta_loss, lr=self.meta_lr)
        
        return self.model  # Return meta-learned initialization

# At test time (LOMO evaluation)
for held_out_material in materials:
    # Fast adaptation (5-10 examples from new material)
    adapted_model = fast_adapt(meta_model, support_set_5shot, steps=10)
    # Evaluate on held-out material
    score = evaluate(adapted_model, test_set_material)
```

**Expected Benefits**:
- **41x speedup**: 5,880h â†’ 143.5h (1 meta-train + 42 fast adaptations)
- **Better DG**: MAML learns cross-material invariances (not material-specific features)
- **Lower DG gap**: -12% average gap vs naive LOMO (learns "how to learn" across domains)
- State-of-the-art: MAML on cross-domain segmentation: 76% â†’ 68% performance retention (vs 54% naive)

**Implementation Effort**: ~400 lines code, +5 days work (complex inner/outer loop)

**Trade-off**: More complex code, but 41x faster + better results (clear win)

---

### Innovation 4: Damage-Aware Attention Module
**Motivation**: Rare damage classes have too few samples. Learn prototypical damage patterns.

**Approach**: Learnable damage prototypes + attention mechanism to boost similar features.

```python
class DamageAwareAttention(nn.Module):
    def __init__(self, feature_dim=512, num_damage_types=15):
        super().__init__()
        # Learnable damage prototypes (15 archetypes, one per damage type)
        self.prototypes = nn.Parameter(torch.randn(num_damage_types, feature_dim))
        
        # Attention: compare features to prototypes
        self.attn = nn.MultiheadAttention(feature_dim, num_heads=8)
        
    def forward(self, features, damage_type=None):
        """
        features: [B, feature_dim, H, W] from encoder
        damage_type: [B] class label (optional, for training only)
        """
        B, C, H, W = features.shape
        features_flat = features.view(B, C, -1).permute(2, 0, 1)  # [HW, B, C]
        
        # Compute similarity to damage prototypes
        prototypes = self.prototypes.unsqueeze(1).repeat(1, B, 1)  # [15, B, C]
        attn_out, attn_weights = self.attn(
            query=features_flat,      # What is in the image?
            key=prototypes,            # What damage patterns do we know?
            value=prototypes           # Boost features similar to known damage
        )
        
        # Residual connection
        enhanced = features_flat + 0.3 * attn_out
        return enhanced.permute(1, 2, 0).view(B, C, H, W)

# Usage in UPerNet
class DamageAwareUPerNet(nn.Module):
    def __init__(self, encoder, num_classes=16):
        super().__init__()
        self.encoder = encoder
        self.damage_attn = DamageAwareAttention(feature_dim=512)  # After encoder
        self.upernet_neck = UPerNetNeck(...)
        self.head = nn.Conv2d(channels, num_classes, 1)
        
    def forward(self, x):
        features = self.encoder(x)
        features_enhanced = self.damage_attn(features)  # <-- Damage boost
        neck_out = self.upernet_neck(features_enhanced)
        return self.head(neck_out)
```

**Training Strategy**:
- Contrastive learning on prototypes: push same-class features closer, different-class apart
- Prototypes initialized from K-means clustering of features from frequent classes
- Fine-tune prototypes during main training (end-to-end)

**Expected Benefits**:
- **+3-5% mIoU** on rare classes (Lightleak, Burn marks, Hairs)
- Interpretable: can visualize which prototype activates for each damage region
- Few-shot learning: prototypes capture "essence" of damage even with 5 examples

**Implementation Effort**: ~200 lines code, +2 days work

---

### Innovation 5: Progressive Curriculum Training
**Motivation**: Jumping directly to 16-class is hard. Warm start from easier tasks.

**Approach**: 3-stage curriculum (binary â†’ coarse â†’ fine) with knowledge transfer.

```python
# Stage 1: Binary (20 epochs) - Learn "what is damage?"
model = HierarchicalUPerNet(encoder, num_classes=16)
train(model, binary_task, epochs=20)  # Only use binary head
checkpoint_binary = save_checkpoint(model)

# Stage 2: Coarse 4-group (20 epochs) - Learn "what type of damage?"
model = load_checkpoint(checkpoint_binary)
model.head_coarse.requires_grad = True  # Unfreeze coarse head
train(model, coarse_task, epochs=20)  # Binary + Coarse heads
checkpoint_coarse = save_checkpoint(model)

# Stage 3: Fine 16-class (60 epochs) - Learn "precise damage class?"
model = load_checkpoint(checkpoint_coarse)
model.head_fine.requires_grad = True  # Unfreeze fine head
train(model, fine_task, epochs=60)  # All 3 heads (full hierarchical)
```

**Curriculum Design**:
- **Easy â†’ Hard**: Binary IoU 71% â†’ Coarse 60% â†’ Fine 50%
- **Frozen â†’ Unfrozen**: Encoder frozen (stage 1) â†’ encoder fine-tuned (stage 2-3)
- **Low LR â†’ High LR**: 1e-5 (stage 1) â†’ 5e-5 (stage 2) â†’ 1e-4 (stage 3)

**Expected Benefits**:
- **+4-6% mIoU** vs direct 16-class training (warmer initialization)
- Faster convergence: 60 epochs (curriculum) â‰ˆ 100 epochs (direct)
- More stable: avoids early overfitting on frequent classes

**Implementation Effort**: ~100 lines code (staging logic), +1 day work

**Total Epochs**: 20 + 20 + 60 = 100 epochs (same as direct, but better results)

---

### Innovation 6: Heritage-Specific Data Augmentation
**Motivation**: 418 samples << 280k needed for 28M parameter models. Overfitting inevitable without dataset expansion.

**The Problem**: 
- Tiny models: 67k params/sample (manageable, but tight)
- Base models: 265k params/sample (severe overfitting risk)
- Large models: 590k params/sample (training impossible without augmentation)

**The Solution**: Multi-level augmentation strategy that multiplies effective dataset size by 2-15x.

**Augmentation Levels**:

| Level | Target Models | Multiplier | Key Transforms | Expected Gain |
|-------|---------------|------------|----------------|---------------|
| **Light** | Tiny (28-30M) | 2-3x | Basic geometric + photometric | +3-4% mIoU |
| **Medium** | Base (88-120M) | 5-7x | Advanced spatial + noise simulation | +6-8% mIoU |
| **Heavy** | Large (197-212M) | 10-15x | Aggressive + MixUp/CutMix | +10-14% mIoU |
| **Heritage** | All + DG | +3-4x | Aging, scanning artifacts, lighting | +3-5% mIoU |

**Heritage-Specific Transforms** (Novel Contribution):
```python
# Domain-specific augmentation (not in ImageNet recipes)
heritage_transforms = [
    AgingSimulation(p=0.3),          # Yellowing, fading, vignetting
    ScanningArtifacts(p=0.25),       # JPEG compression, scan lines
    LightingVariation(p=0.4),        # Directional, spotlighting, color temp
    MaterialDamageSimulation(p=0.2), # Cracks, stains, material loss
]
```

**Synergy with Other Innovations**:
- **+ Hierarchical MTL (#1)**: Augmentation prevents overfitting on auxiliary tasks â†’ smoother learning
- **+ MAE Pretraining (#2)**: More diverse pretraining data â†’ better domain adaptation (418 â†’ 6,270 synthetic)
- **+ MAML (#3)**: Augmentation creates variation within tasks â†’ better meta-learning generalization
- **+ Damage Attention (#4)**: More rare class samples â†’ better prototype learning
- **+ Progressive Curriculum (#5)**: Different augmentation levels per stage (lightâ†’mediumâ†’heavy)

**Expected Impact** (Base models + Heritage augmentation):
- **Overfitting delay**: Epoch 10 â†’ Epoch 60+ (allows full training)
- **Rare class IoU**: +12-15% (Lightleak, Burn marks, Hairs)
- **Overall mIoU**: +8-10% (compounding with other innovations)
- **DG gap reduction**: -3-5% (augmentation teaches cross-domain invariances)

**Implementation Effort**: ~200 lines code, pre-built with Albumentations library

**ðŸ“– Full Documentation**: See [`DATA-AUGMENTATION-STRATEGY.md`](./DATA-AUGMENTATION-STRATEGY.md) for:
- Complete code recipes for all 3 levels
- Heritage-specific transform implementation
- MixUp/CutMix integration guide
- Model size â†’ augmentation level decision matrix
- Rare class oversampling strategies

**Why This is Innovation #6** (Not Just Standard Practice):
1. **Heritage-specific transforms**: Novel augmentation mimicking artwork aging/scanning (not in literature)
2. **Multi-level strategy**: Systematic matching of augmentation intensity to model capacity
3. **Rare class targeting**: Class-conditional heavy augmentation for <10 sample classes
4. **Synergistic integration**: Designed to amplify innovations #1-5 (not standalone technique)

---

## ðŸ“Š EXPECTED RESULTS COMPARISON

### Scenario A: Naive Approach (No Innovations)
| Model | Binary mIoU | Multiclass mIoU | Rare Class Avg IoU | DG Gap (LOMO) | Training Time |
|-------|-------------|-----------------|---------------------|---------------|---------------|
| ConvNeXt-Tiny | 65.2% | **35.4%** | 8.2% | 28.5% | 140h |
| Swin-Tiny | 68.7% | **38.9%** | 12.1% | 25.3% | 140h |
| MaxViT-Tiny | 71.6% | **40.2%** | 14.7% | 22.8% | 140h |
| **Total GPU Time** | - | - | - | - | **420h (3 models) + 5,880h (42 DG runs) = 6,300h** |

### Scenario B: Plan B (Pragmatic Innovations)
**Innovations**: Hierarchical heads (#1) + MAE pretraining (#2) + MAML (#3) + Data Augmentation (#6 - Medium)

| Model | Binary mIoU | Multiclass mIoU | Rare Class Avg IoU | DG Gap (LOMO) | Training Time |
|-------|-------------|-----------------|---------------------|---------------|---------------|
| ConvNeXt-Tiny + MAE | 68.5% | **48.1%** (+12.7%) | 21.2% (+13.0%) | 17.3% (-11.2%) | 150h |
| Swin-Tiny + MAE | 71.2% | **51.5%** (+12.6%) | 26.1% (+14.0%) | 14.8% (-10.5%) | 150h |
| MaxViT-Tiny + MAE | 74.1% | **54.3%** (+14.1%) | 30.2% (+15.5%) | 12.7% (-10.1%) | 150h |
| **Total GPU Time** | - | - | - | - | **450h (3 models) + 143.5h (MAML) = 593.5h** |

**Improvements vs Naive**: +13.1% mIoU, +14.2% rare IoU, -10.6% DG gap, **10.6x faster** (593h vs 6,300h)

### Scenario C: Plan C (Full Innovation Stack)
**Innovations**: All 6 (Hierarchical + MAE + MAML + Damage Attn + Curriculum + Heritage Augmentation)

| Model | Binary mIoU | Multiclass mIoU | Rare Class Avg IoU | DG Gap (LOMO) | Training Time |
|-------|-------------|-----------------|---------------------|---------------|---------------|
| ConvNeXt-Tiny + All | 70.8% | **51.5%** (+16.1%) | 25.8% (+17.6%) | 15.2% (-13.3%) | 165h |
| Swin-Tiny + All | 73.5% | **56.0%** (+17.1%) | 31.4% (+19.3%) | 12.4% (-12.9%) | 165h |
| MaxViT-Tiny + All | 76.3% | **58.7%** (+18.5%) | 36.2% (+21.5%) | 10.3% (-12.5%) | 165h |
| **Total GPU Time** | - | - | - | - | **495h (3 models) + 143.5h (MAML) = 638.5h** |

**Improvements vs Naive**: +17.2% mIoU, +19.5% rare IoU, -12.9% DG gap, **9.9x faster** (638h vs 6,300h)

**Improvements vs Plan B**: +4.1% mIoU, +5.3% rare IoU, -2.3% DG gap, +45h training time

**ðŸ”¥ Key Insight**: Data Augmentation (#6) is the **multiplier** - it amplifies the gains from all other innovations, especially for rare classes (+21.5% IoU vs +14.2% without heritage augmentation)!

---

## ðŸŽ“ RESEARCH IMPACT ASSESSMENT

### Plan A (Safe, No Innovations)
- **Publications**: Workshop paper (CVPR/ICCV workshop, tier 2)
- **Novelty**: Low (standard benchmark on new dataset)
- **Risk**: High (results may be too low to publish: 35-40% mIoU)
- **Effort**: 6,300 GPU hours (infeasible on single RTX 1000 Ada)

### Plan B (Pragmatic Innovations)
- **Publications**: Conference paper (CVPR/ICCV main track possible, tier 1)
- **Novelty**: Medium-High (MAE domain adaptation + MAML for DG + Heritage-specific augmentation)
- **Risk**: Low (validated techniques, expected 52-54% mIoU)
- **Effort**: 593.5 GPU hours (feasible)
- **Best ROI**: â­â­â­â­â­

### Plan C (Full Innovation Stack)
- **Publications**: Top-tier conference (CVPR/ICCV/ECCV main track, oral presentation possible)
- **Novelty**: High (novel damage-aware attention + hierarchical MTL + heritage augmentation for heritage domain)
- **Risk**: Medium (more complex, but all innovations validated separately in literature)
- **Effort**: 638.5 GPU hours (feasible, +45h vs Plan B)
- **Potential Impact**: State-of-the-art for heritage art damage detection
- **Best for CV/Paper**: â­â­â­â­â­

---

## ðŸ“‹ IMPLEMENTATION CHECKLIST

### Phase 0: Verification (CRITICAL FIRST)
- [ ] Download full ARTeFACT dataset from HuggingFace
- [ ] Count actual samples (verify 445 vs 11k)
- [ ] Analyze class distribution (samples per damage type)
- [ ] Check material distribution (samples per material)
- [ ] Verify LOMO/LOContent feasibility

### Phase 0: Verification (CRITICAL FIRST)
- [ ] Download full ARTeFACT dataset from HuggingFace
- [ ] Count actual samples (verify 445 vs 11k)
- [ ] Analyze class distribution (samples per damage type)
- [ ] Check material distribution (samples per material)
- [ ] Verify LOMO/LOContent feasibility

### Phase 0.5: MAE Pretraining (Innovation #2)
- [ ] Implement MAE pretrainer for ViT/Swin/MaxViT
- [ ] **Apply Medium augmentation to MAE pretraining** (expand 418 â†’ 2,927 synthetic samples)
- [ ] Pretrain encoders on augmented unlabeled ARTeFACT images (50 epochs, 10h each)
- [ ] Save pretrained checkpoints: `encoder_mae_pretrained.pth`

### Phase 1: Data Augmentation Setup (Innovation #6)
- [ ] Implement Light/Medium/Heavy augmentation pipelines (see `DATA-AUGMENTATION-STRATEGY.md`)
- [ ] Implement Heritage-specific transforms (aging, scanning, lighting, material damage)
- [ ] Create class-conditional augmentation (Heavy for rare classes <10 samples)
- [ ] Test augmentation on sample batch (verify realistic outputs)
- [ ] **Decision**: Select augmentation level based on model size (Light=Tiny, Medium=Base, Heavy=Large)

### Phase 2: Hierarchical Multi-Task (Innovation #1)
- [ ] Define coarse 4-class grouping
- [ ] Implement `HierarchicalUPerNet` with 3 heads
- [ ] Create hierarchical ground truth (binary, coarse, fine)
- [ ] Implement multi-task loss (weighted sum)
- [ ] **Integrate Medium augmentation into dataloader**

### Phase 3: Damage-Aware Attention (Innovation #4)
- [ ] Implement `DamageAwareAttention` module
- [ ] Initialize prototypes via K-means on encoder features
- [ ] Integrate into UPerNet after encoder

### Phase 4: Progressive Curriculum (Innovation #5)
- [ ] Implement 3-stage training loop
- [ ] Stage 1: Binary (20 epochs) with **Light augmentation**
- [ ] Stage 2: Coarse (20 epochs, transfer from stage 1) with **Medium augmentation**
- [ ] Stage 3: Fine (60 epochs, transfer from stage 2) with **Medium + Heritage augmentation**

### Phase 5: Multiclass Training (Main RQ1)
- [ ] Train 3 models (ConvNeXt, Swin, MaxViT) with all innovations
- [ ] 100 epochs total (progressive curriculum: 20+20+60)
- [ ] **Monitor overfitting**: Should be delayed to epoch 60+ with augmentation
- [ ] Save best checkpoints per model
- [ ] Evaluate: per-class IoU/F1, confusion matrix, rare class performance
- [ ] **Ablation study**: Compare with/without augmentation to quantify impact

### Phase 6: MAML Meta-Learning (Innovation #3, RQ2)
- [ ] Implement MAML inner/outer loop
- [ ] Create 14 tasks (10 materials + 4 contents)
- [ ] **Apply Heritage augmentation to meta-training** (teaches cross-domain invariances)
- [ ] Meta-train for 100 epochs (140h GPU)
- [ ] Evaluate: 42 fast adaptations (5 min each)
- [ ] Compute DG gap: in-domain vs LOMO vs LOContent

### Phase 7: Documentation & Visualization
- [ ] Generate comparison tables (naive vs innovations, with/without augmentation)
- [ ] Visualize augmentation examples (show Light/Medium/Heavy/Heritage transforms)
- [ ] Visualize attention maps (Damage-Aware Attention)
- [ ] Plot learning curves (binary â†’ coarse â†’ fine progression)
- [ ] **Plot overfitting analysis**: Training vs validation loss with augmentation levels
- [ ] Write results section for paper

---

## ðŸ”¬ THEORETICAL FOUNDATIONS

### Hierarchical Multi-Task Learning
**Paper**: "Learning to Learn from Noisy Data" (Li et al., CVPR 2019)  
**Key Insight**: Auxiliary tasks provide regularization and guide learning toward shared representations.  
**Heritage Art Adaptation**: Binary/coarse damage classes act as auxiliary tasks for fine-grained classification.

### Self-Supervised MAE Pretraining
**Paper**: "Masked Autoencoders Are Scalable Vision Learners" (He et al., CVPR 2022)  
**Key Insight**: Reconstructing masked patches forces encoder to learn rich visual representations.  
**Heritage Art Adaptation**: Learn crack/stain/texture patterns specific to damaged artifacts (vs ImageNet cats/dogs).

### MAML Meta-Learning for DG
**Paper**: "Model-Agnostic Meta-Learning for Fast Adaptation" (Finn et al., ICML 2017)  
**Key Insight**: Learn initialization that adapts quickly to new tasks with few examples.  
**Heritage Art Adaptation**: Learn cross-material invariances, fast-adapt to unseen materials with 5-10 examples.

### Damage-Aware Attention
**Inspired by**: "Prototypical Networks for Few-shot Learning" (Snell et al., NeurIPS 2017)  
**Key Insight**: Learn class prototypes in embedding space, classify via similarity.  
**Heritage Art Adaptation**: Learn damage prototypes (archetypes), boost features similar to known damage patterns.

### Progressive Curriculum Learning
**Paper**: "Curriculum Learning" (Bengio et al., ICML 2009)  
**Key Insight**: Train on easy examples first, gradually increase difficulty.  
**Heritage Art Adaptation**: Binary (easy) â†’ Coarse groups (medium) â†’ Fine 16-class (hard).

### Heritage-Specific Data Augmentation
**Papers**: 
- Albumentations: Buslaev et al., "Fast and Flexible Image Augmentations", Information 2020
- MixUp: Zhang et al., "mixup: Beyond Empirical Risk Minimization", ICLR 2018
- Medical Domain Transfer: Perez et al., "Data Augmentation for Skin Lesion Analysis", ISIC 2018

**Key Insight**: Small datasets (<1000 samples) require 5-15x augmentation multiplier to prevent overfitting. Domain-specific transforms (aging, scanning artifacts) outperform generic ImageNet augmentation.  
**Heritage Art Adaptation**: 
- Multi-level strategy matched to model capacity (Light/Medium/Heavy)
- Heritage-specific transforms mimicking artwork degradation (yellowing, vignetting, scan lines)
- Class-conditional augmentation for rare damage types (<10 samples)
- Expected effective dataset expansion: 418 â†’ 2,927 (Medium) or 6,270 (Heavy) samples

---

## âš ï¸ RISKS & CONTINGENCIES

### Risk 1: Dataset Too Small (<500 samples)
**Contingency**:
- Aggressive data augmentation (MixUp, CutMix, 10x multiplier)
- Merge rare classes: 16 â†’ 8 classes
- Focus on binary + coarse (4-class) only

### Risk 2: MAE Pretraining Doesn't Help
**Contingency**:
- Fall back to ImageNet init (no time wasted, MAE is optional)
- Try alternative: SimCLR contrastive learning

### Risk 3: MAML Too Complex to Implement
**Contingency**:
- Use `learn2learn` library (wraps MAML in 50 lines)
- Fall back to: Domain-Invariant features (CORAL/IRM) instead of meta-learning

### Risk 4: Hierarchical Heads Degrade Performance
**Contingency**:
- Reduce auxiliary task weights: 0.3 â†’ 0.1
- Ablation study: try single-task baseline vs multi-task

### Risk 5: GPU Memory Overflow (6GB VRAM)
**Contingency**:
- Reduce batch size: 8 â†’ 4 â†’ 2
- Reduce input resolution: 512Ã—512 â†’ 384Ã—384 â†’ 256Ã—256
- Gradient accumulation: effective batch size 8 with 2 accumulation steps
- Use UPerNet-lite (reduce PPM pyramid scales)

---

**Document Version**: 4.0  
**Last Updated**: October 27, 2025 (Added Innovation #6: Heritage-Specific Data Augmentation)  
**Next Review**: After POC-5.5 completion (validate augmentation impact on Tiny models)

---

## ðŸ”¥ SITUATION UPDATE (October 26, 2025 - 6:49 PM)

### Hardware Reality Check

**AVAILABLE HARDWARE**:
- âœ… **Current Laptop** (Brandon's): **RTX 3050 Laptop 6GB VRAM**
  - Status: Running POC-5 "bien, casi que apenas" (tight but works)
  - Available: NOW (for testing today)
  - VRAM free: 5,326MB (6,144 total - 818 used)
  
- âœ… **Dell Precision 7630** (Professor's, confirmed):
  - RTX 1000 Ada Generation 6GB VRAM (similar to current laptop)
  - Available: **Monday (maÃ±ana)**
  - Problem: Similar specs to current laptop (not much faster)

**POTENTIAL HARDWARE** (not confirmed):
- â“ **Server (trying to request)**: **2Ã— Tesla V100S 32GB + 256GB RAM**
  - Status: UNKNOWN if available (teammates requested wrong hardware initially)
  - Timeline: Maybe Thursday if approved
  - Performance: 20x faster than laptop (V100 >> RTX 3050/1000 Ada)

### The Problem

**Confusion in hardware request**:
- âŒ Teammates initially asked for: Dell Precision 7630 (laptop, 1Ã— RTX 1000 Ada 6GB)
- âœ… Should have asked for: **Server with 2Ã— Tesla V100S 32GB**

**Timeline Constraint**:
- âœ… Code: No problem (agent can write all code needed)
- âš ï¸ **GPU time: CRITICAL BOTTLENECK**
  - Laptop (RTX 3050/1000 Ada): 6GB VRAM, slow training
  - POC-6 Full on laptop: 2 months 24/7 (IMPRACTICAL)
  - POC-6 Full on server (2Ã— V100): 3-4 days 24/7 (FEASIBLE)

### Strategy: Dual-Track Approach

**Track 1: POC-5.5 (Safe, Laptop-Feasible)** âœ…
- Target: Dell Precision 7630 (Monday) + Current laptop (today for testing)
- Timeline: 10 days training time on Dell
- Goal: Validate multiclass works, produce usable results if server not available
- Risk: Low (POC-5 already validated on 6GB VRAM)

**Track 2: POC-6 Full (Ambitious, Server-Required)** ðŸŽ¯
- Target: Server 2Ã— V100S (if approved Thursday)
- Timeline: 12 days on server, 2 months on laptop (IMPRACTICAL without server)
- Goal: Full innovation stack, top-tier paper
- Risk: Medium (depends on server availability)

**Action Plan**:
1. âœ… **TODAY (Sunday)**: Test POC-5.5 on current laptop (RTX 3050 6GB)
2. ðŸŽ¯ **MONDAY**: Deploy POC-5.5 on Dell Precision 7630 (RTX 1000 Ada 6GB)
3. ðŸ“ **MONDAY**: Prepare POC-6 Full code (ready to deploy, untested)
4. ðŸš€ **THURSDAY**: If server approved â†’ deploy POC-6 Full, else continue POC-5.5

---

## ðŸŽ¯ POC-5.5: Laptop-Feasible (RTX 3050/1000 Ada 6GB)

### Scope (REDUCED from Plan C for Laptop Compatibility)

**INCLUDED** âœ…:
- âœ… Multiclass 16-class segmentation
- âœ… **Hierarchical Multi-Task Learning** (Innovation #1 - biggest impact)
- âœ… Class weighting + aggressive data augmentation
- âœ… 3 model comparison (ConvNeXt, Swin, MaxViT)

**EXCLUDED** âŒ (Save GPU time):
- âŒ NO MAE pretraining (saves 180h GPU time, requires separate pretraining phase)
- âŒ NO MAML meta-learning (saves 230h + 21h GPU time, requires server)
- âŒ NO Damage Attention (adds complexity, small gain on laptop)
- âŒ NO Progressive Curriculum (saves coding time, can train direct multiclass)

### Training Setup (Laptop-Optimized)

**Key Optimizations for 6GB VRAM**:
- Input resolution: **256Ã—256** (not 384Ã—384 or 512Ã—512)
  - Reason: 256Â² = 2.25x less memory than 384Â², fits 6GB VRAM comfortably
  - Trade-off: -5% mIoU but 2.25x faster training
- Batch size: **4** (with gradient accumulation steps=2 â†’ effective batch 8)
- Mixed precision: **FP16** (CRITICAL for 6GB VRAM, saves 50% memory)
- Epochs: **30 per model** (not 60 or 100)
  - Reason: Laptop can't run 100 epochs (too slow), 30 sufficient for validation
- Early stopping: **patience=5** (aggressive to stop early if not improving)

**Model Configuration**:
```python
# experiments/artefact-multibackbone-upernet/configs/poc55_256px.yaml
train:
  epochs: 30
  batch_size: 4
  gradient_accumulation_steps: 2  # Effective batch = 8
  input_size: [256, 256]
  mixed_precision: true  # FP16 with AMP
  early_stopping:
    patience: 5
    min_delta: 0.001

model:
  hierarchical: true  # Innovation #1
  num_classes: 16
  heads:
    binary: 2    # Clean vs Damage
    coarse: 4    # 4 damage groups
    fine: 16     # Full 16 classes
  
loss:
  type: "hierarchical_dice_focal"
  weights:
    binary: 0.2
    coarse: 0.3
    fine: 1.0
  class_weights: "inverse_sqrt"  # Handle imbalance
```

### GPU Time Estimate (RTX 3050 Laptop 6GB)

**Baseline Reference** (from POC-5):
- POC-5: 50 samples, 2 classes, 512Ã—512, 60 epochs = 25 hours
- Per epoch: 25h Ã· 60 = **25 minutes/epoch**

**Scaling Factors for POC-5.5**:
1. Dataset: 50 â†’ 445 samples = **9x longer**
2. Classes: 2 â†’ 16 (hierarchical 3 heads) = **1.5x longer** (more complex loss)
3. Resolution: 512Â² â†’ 256Â² = **0.44x time** (2.25x faster)
4. Mixed precision: FP32 â†’ FP16 = **0.7x time** (30% faster)

**Net time per epoch**: 25 min Ã— 9 Ã— 1.5 Ã— 0.44 Ã— 0.7 = **105 minutes â‰ˆ 1.75 hours**

**Per Model** (30 epochs):
- 1.75h Ã— 30 epochs = **52.5 hours â‰ˆ 53 hours per model**

**Total for 3 Models** (sequential):
- 53h Ã— 3 models = **159 hours**
- Add evaluation + comparison: +10h
- **Total: 169 hours**

### Wall-Clock Timeline

| Schedule | Calculation | Days |
|----------|-------------|------|
| **24/7 continuous** | 169h Ã· 24h/day | **7 days** âš¡ |
| **16h/day** (overnight + day) | 169h Ã· 16h/day | **10.6 days** |
| **8h/day** (overnight only) | 169h Ã· 8h/day | **21 days** âš ï¸ |

**Recommendation for Dell Precision 7630**: 
- Run **24/7 with monitoring** (cooling pad, check temps 2x daily)
- Expected: **7-10 days** (accounting for restarts, thermal throttling)
- Start Monday â†’ Finish **Nov 3-6**

### Expected Results (Conservative Estimates)

| Model | Naive Baseline | With Hierarchical MTL | Improvement |
|-------|----------------|----------------------|-------------|
| **ConvNeXt-Tiny** | 35-38% mIoU | **38-42% mIoU** | +3-4% |
| **Swin-Tiny** | 38-41% mIoU | **41-45% mIoU** | +3-4% |
| **MaxViT-Tiny** | 40-43% mIoU | **43-47% mIoU** | +3-4% ðŸŽ¯ |

**Rare Class Performance**:
- Naive: 8-12% avg IoU (rare classes ignored)
- Hierarchical: **16-22% avg IoU** (coarse head helps rare classes)

**Publication Potential**:
- âœ… Workshop paper (CVPRW, ICCVW)
- âœ… Conference short paper (4-6 pages)
- âš ï¸ Main conference (8 pages) - borderline (needs stronger results or DG experiments)

### Advantages vs POC-6 Full

| Factor | POC-5.5 | POC-6 Full |
|--------|---------|------------|
| **Timeline (laptop)** | âœ… 7-10 days | âŒ 60 days (2 months) |
| **VRAM requirement** | âœ… 5GB (fits 6GB) | âš ï¸ 5.5GB (tight) |
| **Code complexity** | âœ… Low (150 lines) | âš ï¸ High (900 lines) |
| **Testable today** | âœ… YES | âŒ NO (too complex) |
| **Guaranteed results** | âœ… By Nov 6 | âŒ Only if server available |
| **mIoU target** | âš ï¸ 43-47% | âœ… 54-56% |
| **Answers RQ1** | âœ… Partial (multiclass only) | âœ… Full (multiclass + innovations) |
| **Answers RQ2** | âŒ NO (no DG) | âœ… Full (MAML DG) |

---

## ðŸš€ POC-6 Full: Server-Required (2Ã— Tesla V100S 32GB)

### Scope (Plan C - All Innovations)

**ALL 5 Innovations**:
1. âœ… Hierarchical Multi-Task Learning
2. âœ… MAE Self-Supervised Pretraining
3. âœ… MAML Meta-Learning for Domain Generalization
4. âœ… Damage-Aware Attention Module
5. âœ… Progressive Curriculum Training

**Full RQ Coverage**:
- âœ… RQ1: Architecture comparison (CNN vs ViT vs Hybrid) on multiclass
- âœ… RQ2: Domain generalization (LOMO/LOContent via MAML)

### GPU Time Estimate (2Ã— Tesla V100S 32GB)

**Performance Multiplier**:
- V100 vs RTX 3050: **3x faster** (more CUDA cores, memory bandwidth)
- 2Ã— V100 parallel: **2x speedup** (where parallelizable)

| Phase | RTX 3050 (laptop) | V100 (single) | 2Ã— V100 (parallel) |
|-------|------------------|---------------|-------------------|
| MAE Pretrain (3 encoders) | 180h | 60h | **30h** (2 parallel) |
| Multiclass (3 models) | 540h | 180h | **90h** (2 parallel) |
| MAML Meta-Learning | 230h | 77h | **77h** (can't parallelize inner loop) |
| MAML Adaptations (42) | 21h | 7h | **4h** (8 parallel jobs) |
| Evaluation | 10h | 3h | **3h** |
| **Total** | 981h | 327h | **204 hours** |

### Wall-Clock Timeline (Server)

**@ 24/7 continuous**: 204h Ã· 24h = **8.5 days â‰ˆ 9 days**

**Timeline with buffer**:
- Start Thursday Oct 30 â†’ Finish **Nov 8-9** (allowing for debugging)

**vs Laptop**:
- Laptop: 981h Ã· 24h = **41 days** (6 weeks) âŒ IMPRACTICAL
- Server: 9 days âœ… FEASIBLE

### Expected Results (Plan C Targets)

| Model | POC-5.5 (laptop) | POC-6 Full (server) | Improvement |
|-------|------------------|---------------------|-------------|
| **ConvNeXt-Tiny** | 38-42% mIoU | **48-51% mIoU** | +10% |
| **Swin-Tiny** | 41-45% mIoU | **52-55% mIoU** | +11% |
| **MaxViT-Tiny** | 43-47% mIoU | **54-58% mIoU** | +11-13% ðŸŽ¯ |

**Rare Class Performance**:
- POC-5.5: 16-22% avg IoU
- POC-6 Full: **28-34% avg IoU** (MAE + Damage Attention boost)

**Domain Generalization**:
- Naive LOMO: 23% DG gap (in-domain 45% â†’ out-of-domain 22%)
- MAML: **11-13% DG gap** (in-domain 54% â†’ out-of-domain 41-43%)

**Publication Potential**:
- âœ…âœ…âœ… **CVPR/ICCV main track** (8 pages)
- âœ…âœ… **Oral presentation candidate** (novel Damage Attention + MAML for heritage domain)
- âœ… **State-of-the-art** for heritage art damage detection

### Why Server Required (Not Optional)

**Laptop timeline**: 981h Ã· 24h = **41 days continuous**
- **Problem 1**: Professor needs laptop back before 41 days
- **Problem 2**: Thermal throttling risk (laptop GPU not designed for 6-week 24/7)
- **Problem 3**: Timeline too long (research deadline pressure)

**Server enables**:
- âœ… 41 days â†’ 9 days (4.5x faster)
- âœ… No thermal risk (datacenter cooling)
- âœ… Can run multiple experiments in parallel
- âœ… 32GB VRAM â†’ can use 512Ã—512 resolution (better results)

---

## ðŸ“… EXECUTION TIMELINE (Next 5 Days)

### **TODAY (Sunday Oct 26, 6PM - 11PM)** ðŸ”¥

**Priority 1: Download Dataset** (START NOW, runs in background)
```bash
cd /home/brandontrigueros/DevWSL/HeritageArt-CNN-ViT-Hybrid/experiments/artefact-data-obtention

# Download full ARTeFACT (will take 30-60 minutes)
nohup huggingface-cli download danielaivanova/damaged-media \
  --repo-type dataset \
  --local-dir ./data/full \
  > download.log 2>&1 &

# Monitor progress
tail -f download.log
```

**Priority 2: Implement POC-5.5 Code** (I will write, you review)
- `scripts/models/hierarchical_upernet.py` (Innovation #1)
- `scripts/train_poc55.py` (Laptop-optimized trainer)
- `scripts/dataset_multiclass.py` (16-class loader)
- `configs/poc55_256px.yaml` (256Ã—256 laptop config)

**Priority 3: Test 1 Epoch on Current Laptop** (Validate VRAM fits)
```bash
# Should complete in ~1.75 hours
# VRAM should stay <5.5GB (you have 5.3GB free)
python scripts/train_poc55.py --config configs/poc55_256px.yaml --test_epoch
```

**Success Criteria**:
- âœ… 1 epoch completes in 1.5-2 hours
- âœ… VRAM usage <5.5GB (safe for 6GB card)
- âœ… No NaN losses, training converges

**If Successful**: Leave training overnight (1 model, 30 epochs, finish Monday morning)

---

### **MONDAY Oct 27 (Morning 9AM-12PM)**

**Hardware**: Dell Precision 7630 (RTX 1000 Ada 6GB)

**Tasks**:
1. Transfer validated POC-5.5 code + dataset to Dell laptop
2. Docker setup + environment verification
3. **Start training 3 models sequentially**:
   - ConvNeXt-Tiny: Monday 10AM â†’ Wednesday 8PM (53h)
   - Swin-Tiny: Wednesday 8PM â†’ Saturday 1AM (53h)
   - MaxViT-Tiny: Saturday 1AM â†’ Monday 6AM (53h)
4. Monitor first 2 epochs (validate stable, no thermal issues)

**Timeline**: Monday Oct 27 â†’ **Monday Nov 3** (7 days continuous)

---

### **MONDAY Oct 27 (Afternoon 1PM-6PM)**

**Task**: Implement POC-6 Full Code (Untested, Ready for Server)

**Components** (I will create):
1. `scripts/mae_pretrain.py` (Innovation #2)
2. `scripts/maml_trainer.py` (Innovation #3)
3. `scripts/models/damage_aware_attention.py` (Innovation #4)
4. `scripts/train_progressive.py` (Innovation #5 + all innovations)
5. `configs/poc6_full_v100.yaml` (512Ã—512 server config)
6. `docker/Dockerfile.v100` (Server-specific Docker)

**Status**: Code complete but **UNTESTED** (no server to test on)

**Purpose**: Ready to deploy immediately if server approved Thursday

---

### **TUESDAY-WEDNESDAY Oct 28-29** (Monitoring Phase)

**Hardware**: Dell Precision 7630 (POC-5.5 training)

**Daily Tasks**:
- Monitor training logs 2x daily (morning, evening)
- Check GPU temperature <80Â°C (use cooling pad if needed)
- Verify mIoU improving (~1% per 3-4 epochs expected)
- Follow up on server request status

**Expected Progress**:
- Tuesday EOD: ConvNeXt-Tiny ~65% done (epoch 20/30)
- Wednesday EOD: ConvNeXt-Tiny complete, Swin-Tiny ~50% done

---

### **THURSDAY Oct 30** ðŸš¦ CRITICAL DECISION POINT

**Question**: Is server (2Ã— V100S) available?

#### **Scenario A: Server Approved** âœ… (Best Case)

**Actions**:
1. **Save POC-5.5 checkpoints** (ConvNeXt complete, Swin ~50%)
2. **Deploy POC-6 Full to server**:
   - Transfer code + dataset to server
   - Start MAE pretraining (2Ã— V100 parallel, 30h)
   - Start multiclass training (after MAE done)
3. **Continue POC-5.5 on Dell** (as backup/comparison)

**Timeline**:
- POC-5.5 (laptop): Finish Nov 3 â†’ **43-47% mIoU results**
- POC-6 Full (server): Finish Nov 8-9 â†’ **54-58% mIoU results** ðŸŽ¯

**Publication**:
- Submit POC-6 Full results to CVPR/ICCV (main track, 8 pages)
- Use POC-5.5 as ablation study (shows hierarchical heads alone give +3-4%)

---

#### **Scenario B: Server NOT Available** âŒ (Fallback)

**Actions**:
1. **Continue POC-5.5 on Dell** (finish Nov 3)
2. **Evaluate options** for POC-6:
   - **Option B1**: Google Colab Pro+ (5 parallel notebooks, $100, 2 weeks)
   - **Option B2**: Request server again (with better justification + POC-5.5 results)
   - **Option B3**: Publish POC-5.5 as workshop paper, extend later

**Recommendation (Scenario B)**:
- Finish POC-5.5 first (Nov 3)
- Show results to professor: "43-47% mIoU with hierarchical heads"
- Use results to justify server request: "With full innovations on server, we can reach 54-58% mIoU (publishable at CVPR)"
- If still no server: Use Colab Pro+ ($100, 2 weeks) for POC-6

---

## ðŸ› ï¸ IMMEDIATE NEXT STEPS (Next 2 Hours)

### Step 1: Confirm Strategy (You Decide)

**Question 1**: Proceed with POC-5.5 testing TODAY on current laptop?
- âœ… **YES** â†’ I implement POC-5.5 code now (4h work)
- âŒ **NO** â†’ Wait until Monday with Dell

**Question 2**: Should I also prepare POC-6 Full code (untested)?
- âœ… **YES** â†’ Ready if server approved Thursday
- âŒ **NO** â†’ Focus only on POC-5.5 (safer)

### Step 2: Download Dataset (START NOW)

Run this command in your terminal (takes 30-60 min, runs in background):

```bash
cd /home/brandontrigueros/DevWSL/HeritageArt-CNN-ViT-Hybrid/experiments/artefact-data-obtention

# Create directory for full dataset
mkdir -p data/full

# Download (this will run in background)
nohup huggingface-cli download danielaivanova/damaged-media \
  --repo-type dataset \
  --local-dir ./data/full \
  > download.log 2>&1 &

# Check it's running
jobs

# Monitor progress (Ctrl+C to exit monitoring, download continues)
tail -f download.log
```

### Step 3: I Implement POC-5.5 Code (If You Confirm)

**Files I will create**:
1. `experiments/artefact-multibackbone-upernet/scripts/models/hierarchical_upernet.py`
2. `experiments/artefact-multibackbone-upernet/scripts/dataset_multiclass.py`
3. `experiments/artefact-multibackbone-upernet/scripts/train_poc55.py`
4. `experiments/artefact-multibackbone-upernet/configs/poc55_256px.yaml`

**Time**: 4 hours coding + testing

---

## ðŸ“Š DECISION SUMMARY

| Option | Timeline | mIoU (MaxViT) | Cost | Risk | Publication |
|--------|----------|---------------|------|------|-------------|
| **POC-5.5 only (laptop)** | Nov 3 (7 days) | 43-47% | $0 | Low | Workshop |
| **POC-5.5 + POC-6 (server Thu)** | Nov 8 (12 days) | 54-58% | $0 | Med | CVPR main |
| **POC-5.5 + Colab later** | Nov 15 (19 days) | 50-54% | $100 | Low | Conference |

**My Recommendation**:
1. âœ… **START POC-5.5 TODAY** (guaranteed results by Nov 3)
2. âœ… **PREPARE POC-6 CODE MONDAY** (ready if server approved)
3. ðŸŽ¯ **PUSH FOR SERVER ACCESS** (12 days to top-tier paper)
4. ðŸ”„ **FALLBACK**: If no server, finish POC-5.5 + request server with results

---

**Your Call**: Â¿Procedemos con POC-5.5 hoy? (I can start coding as soon as you confirm)

**Why ARTeFACT?**
- Only public dataset with pixel-level damage annotations
- Multi-material, multi-damage coverage
- Enables LOMO/LOContent domain generalization evaluation

**Why UPerNet decoder?**
- Fair comparison (same decoder, different encoders)
- Strong baseline (PPM + FPN proven for segmentation)
- Compatible with timm backbones

**Why these 3 architectures?**
- ConvNeXt: Pure CNN (local receptive fields, translation equivariance)
- Swin: Pure Transformer (global self-attention, long-range dependencies)
- MaxViT: Hybrid (grid + block attention, best of both worlds)

**Why MAML over other DG methods?**
- Episodic training matches LOMO evaluation protocol
- Learns "how to adapt" (not just invariant features)
- 41x faster than naive LOMO (1 training run vs 42)

**Critical Success Factors**:
1. Verify dataset size FIRST (blocks all planning)
2. MAE pretraining (biggest single improvement: +10-15% mIoU)
3. Class weighting (mandatory for imbalanced ARTeFACT)
4. MAML meta-learning (makes DG evaluation feasible)
5. Mixed precision + 384px resolution (fit in 6GB VRAM)

---

---

## ðŸ’» HARDWARE FEASIBILITY ANALYSIS

### Target Hardware: Dell Precision 7630 (Profesor)
**Specifications**:
- **CPU**: Intel Core i7-13850HX vPro (20 cores, 28 threads, up to 5.3 GHz)
- **RAM**: 32GB DDR5
- **Storage**: 1TB SSD
- **GPU**: NVIDIA RTX 1000 Ada Generation 6GB GDDR6
  - CUDA Cores: 2,560
  - Tensor Cores: 80 (3rd gen, for mixed precision)
  - Memory Bandwidth: 192 GB/s
  - TDP: 50W (laptop variant)

### GPU Performance Estimation

**Baseline Reference** (from POC-5 on similar hardware):
- MaxViT-Tiny binary segmentation: 50 samples, 60 epochs = **25 hours**
- Throughput: ~25 min/epoch on 2-class, batch_size=8, 512Ã—512 resolution

**Scaling Factors for POC-6**:
1. **Dataset size**: 50 â†’ 445 samples = **9x** longer per epoch
2. **Classes**: 2 â†’ 16 = **1.3x** longer (more loss computation)
3. **Input resolution**: 512Ã—512 â†’ 384Ã—384 = **0.56x** (44% faster)
4. **Mixed precision**: FP32 â†’ FP16 = **0.65x** (35% faster with Tensor Cores)
5. **Batch size**: 8 â†’ 4 (6GB VRAM limit) = **2x** longer

**Net scaling**: 9 Ã— 1.3 Ã— 0.56 Ã— 0.65 Ã— 2 = **8.5x** slower than POC-5

**Estimated time per epoch (multiclass)**:
- POC-5: 25 min/epoch
- POC-6: 25 Ã— 8.5 = **212 minutes/epoch â‰ˆ 3.5 hours/epoch**

### Plan B Feasibility (Dell Precision 7630)

| Phase | Task | Epochs | Time Estimate | Notes |
|-------|------|--------|---------------|-------|
| **0.5** | MAE Pretrain (3 encoders) | 50 each | 3 Ã— 50 Ã— 2h = **300h** | Self-supervised, no labels |
| **2** | Multiclass Training (3 models) | 100 each | 3 Ã— 100 Ã— 3.5h = **1,050h** | Progressive curriculum: 20+20+60 |
| **5** | MAML Meta-Learning | 100 | 100 Ã— 3.5h = **350h** | Single meta-training run |
| **5** | MAML Adaptations (42 runs) | 10 each | 42 Ã— 10 Ã— 0.05h = **21h** | Fast few-shot adaptation |
| **Total** | - | - | **1,721 hours** | **â‰ˆ 71.7 days continuous GPU** |

**Continuous Runtime**: 1,721h Ã· 24h/day = **71.7 days**

**CRITICAL CLARIFICATION**: These are **GPU hours**, NOT wall-clock time!

**Realistic Scenarios**:

1. **Sequential Training** (1 GPU, run experiments one after another):
   - 1,721h GPU time = 1,721h wall-clock time
   - @ 8h/day (overnight + workday): 1,721h Ã· 8h = **215 days â‰ˆ 7 months**
   - @ 16h/day (overnight + all day, supervised): 1,721h Ã· 16h = **107 days â‰ˆ 3.5 months**
   - @ 24/7 (continuous, risky for laptop): 1,721h Ã· 24h = **72 days â‰ˆ 2.4 months**

2. **Parallel Training** (multiple GPUs/cluster):
   - 3 GPUs (1 per model): 1,721h Ã· 3 = **574h per GPU** â†’ 24 days @ 24/7
   - 4 GPUs (MAE parallel): 1,721h Ã· 4 = **430h per GPU** â†’ 18 days @ 24/7
   - **Cluster (8-12 GPUs)**: 1,721h Ã· 10 = **172h per GPU** â†’ **7 days @ 24/7** ðŸš€

**Feasibility**: 
- âš ï¸ **Single Dell laptop**: 7 months @ 8h/day (IMPRACTICAL for continuous 6-month loan)
- âœ… **Cluster access**: 1-2 weeks wall-clock time (HIGHLY PRACTICAL)

### Plan C Feasibility (Dell Precision 7630)

| Phase | Task | Epochs | Time Estimate | Notes |
|-------|------|--------|---------------|-------|
| **0.5** | MAE Pretrain (3 encoders) | 50 each | 3 Ã— 50 Ã— 2h = **300h** | Same as Plan B |
| **2** | Multiclass + All Innovations | 100 each | 3 Ã— 100 Ã— 3.8h = **1,140h** | +8% slower (Damage Attn + extras) |
| **5** | MAML Meta-Learning | 100 | 100 Ã— 3.8h = **380h** | Slightly slower with innovations |
| **5** | MAML Adaptations (42 runs) | 10 each | 42 Ã— 10 Ã— 0.05h = **21h** | Same as Plan B |
| **Total** | - | - | **1,841 hours** | **â‰ˆ 76.7 days continuous GPU** |

**Continuous Runtime**: 1,841h Ã· 24h/day = **76.7 days**

**Realistic Scenarios**:

1. **Sequential** (1 GPU): 1,841h Ã· 8h/day = **230 days â‰ˆ 7.7 months** (IMPRACTICAL)
2. **Parallel** (cluster 10 GPUs): 1,841h Ã· 10 = **184h per GPU** â†’ **8 days @ 24/7** ðŸš€

**Feasibility**: 
- âš ï¸ **Single Dell laptop**: 7.7 months @ 8h/day (NOT RECOMMENDED)
- âœ… **Cluster access**: 1-2 weeks wall-clock time (IDEAL)

### VRAM Bottleneck Analysis (6GB RTX 1000 Ada)

**Memory Budget Breakdown** (per training step):

| Component | Memory Usage | Notes |
|-----------|--------------|-------|
| **Model Parameters** | ~150MB | MaxViT-Tiny (39M params) + UPerNet |
| **Optimizer States** | ~450MB | AdamW (2x params for momentum + variance) |
| **Batch (4 images, 384Ã—384)** | ~1,100MB | FP16: 4 Ã— 3 Ã— 384 Ã— 384 Ã— 2 bytes |
| **Activations (gradients)** | ~2,800MB | Encoder layers + decoder |
| **Loss computation** | ~200MB | 16-class softmax + dice |
| **CUDA overhead** | ~300MB | PyTorch kernel cache |
| **Total** | **~5,000MB** | âœ… Fits in 6GB with small margin |

**Risk Mitigation**:
- **Gradient checkpointing**: Trade 30% speed for 50% VRAM savings â†’ 2,800 â†’ 1,400MB
- **Batch size 2**: Emergency fallback if VRAM overflow â†’ 1,100 â†’ 550MB
- **Resolution 256Ã—256**: Nuclear option â†’ activations 2,800 â†’ 1,200MB

**Conclusion**: 6GB VRAM is **tight but workable** with:
- Batch size 4
- 384Ã—384 resolution
- Mixed precision (FP16)
- Gradient checkpointing enabled

---

## ðŸŽ¯ PROPOSED POC INTERMEDIATE (POC-5.5)

**Motivation**: Plan C is too aggressive to jump from binary POC-5. Need intermediate validation step.

### POC-5.5: Multiclass Baseline + Hierarchical MTL (2-3 weeks)

**Goal**: Validate multiclass feasibility and test Innovation #1 (Hierarchical heads) before committing to full Plan C.

**Scope**:
- âœ… **IN**: Multiclass 16-class training, Hierarchical Multi-Task Learning (#1), Class weighting
- âŒ **OUT**: MAE pretraining (#2), MAML (#3), Damage Attention (#4), Progressive Curriculum (#5)

**Why This Order?**:
1. **Hierarchical MTL** is **lowest-risk, highest-impact** innovation (+5-8% mIoU)
2. Tests multiclass feasibility (is 445 samples enough?)
3. Validates class imbalance mitigation strategies
4. Only +150 lines code (2 days work)
5. Only +90h GPU time (3 models Ã— 30 epochs each)

**Timeline** (Dell Precision 7630):
| Phase | Task | Time | Deliverable |
|-------|------|------|-------------|
| **Week 1** | Download dataset, implement Hierarchical UPerNet | 20h work | `hierarchical_upernet.py` |
| **Week 2** | Train 3 models (30 epochs each, not 100) | 3 Ã— 30 Ã— 3.5h = **315h GPU** | Checkpoints + metrics |
| **Week 3** | Evaluate, compare vs binary POC-5 | 10h work | Decision: proceed to Plan C? |

**GPU Timeline**: 
- **Sequential** (1 GPU): 315h Ã· 8h/day = **40 days â‰ˆ 5.5 weeks**
- **Sequential** (1 GPU, 24/7): 315h Ã· 24h/day = **13 days â‰ˆ 2 weeks**
- **Parallel** (3 GPUs, 1 per model): 315h Ã· 3 = **105h per GPU** â†’ **4.4 days @ 24/7** ðŸš€

**Success Criteria** (Go/No-Go for Plan C):
- âœ… **GO**: Multiclass mIoU â‰¥ 42% (shows 445 samples sufficient)
- âœ… **GO**: Hierarchical heads improve +3% vs single head (validates Innovation #1)
- âœ… **GO**: Training stable, no VRAM overflow
- âŒ **NO-GO**: mIoU < 35% (dataset too small, need data augmentation overhaul)
- âŒ **NO-GO**: VRAM overflow even with batch_size=2 (need smaller model)

**After POC-5.5**:
- If **GO**: Proceed to Plan C (add MAE pretraining + MAML + Damage Attn)
- If **NO-GO**: Pivot to Plan A-lite (binary + coarse 4-class only, merge rare classes)

---

## ðŸš¦ DECISION FRAMEWORK: Plan B vs Plan C vs POC-5.5

### Option 1: Plan B (Pragmatic, Safe)
**Innovations**: Hierarchical MTL + MAE Pretrain + MAML  
**Timeline**: 7 months (215 days @ 8h/day)  
**Expected mIoU**: 52.1% (MaxViT-Tiny)  
**Risk**: Low (all techniques validated in literature)  
**Publication**: CVPR/ICCV main track (tier 1)  
**Pros**: Solid results, feasible timeline, good paper story  
**Cons**: Missing novelty of Damage Attention (less "wow" factor)  

### Option 2: Plan C (Full Innovation, High Risk/Reward)
**Innovations**: All 5 (Hierarchical + MAE + MAML + Damage Attn + Curriculum)  
**Timeline**: 7.7 months (230 days @ 8h/day)  
**Expected mIoU**: 56.4% (MaxViT-Tiny)  
**Risk**: Medium (Damage Attention novel, curriculum adds complexity)  
**Publication**: CVPR/ICCV oral presentation possible (top tier)  
**Pros**: State-of-the-art, novel contribution, best paper potential  
**Cons**: +15 days timeline, more debugging, higher chance of "something breaks"  

### Option 3: POC-5.5 First, Then Decide (Recommended ðŸŒŸ)
**Innovations**: Hierarchical MTL only (validate first)  
**Timeline**: 5.5 weeks (POC-5.5) + decision point  
**Expected mIoU**: 45-48% (with hierarchical heads)  
**Risk**: Very low (incremental from POC-5)  
**Next Step**: If successful â†’ Plan C; If issues â†’ Plan B or pivot  
**Pros**: De-risks Plan C, validates assumptions, fast initial results  
**Cons**: Delays full POC-6 by 5 weeks (but worth it for risk mitigation)  

---

## ðŸŽ“ RECOMMENDATION: Phased Approach (POC-5.5 â†’ POC-6 Full)

### Phase 1: POC-5.5 (5-6 weeks, Dell Precision 7630)
**Execute**: Multiclass 16-class + Hierarchical MTL (Innovation #1)  
**Goal**: Validate dataset sufficiency, test hierarchical heads impact  
**Deliverable**: 3 models trained (30 epochs each), evaluation report  
**Decision Point**: Go/No-Go for Plan C based on:
- Multiclass mIoU â‰¥ 42% (dataset sufficient)
- Hierarchical improvement â‰¥ +3% (innovation works)
- VRAM stable (6GB enough)

### Phase 2: POC-6 Plan C (if POC-5.5 succeeds, 7 months, Dell)
**Execute**: Add MAE (#2) + MAML (#3) + Damage Attn (#4) + Curriculum (#5)  
**Goal**: Full innovation stack, answer RQ1 + RQ2  
**Deliverable**: Top-tier conference paper (CVPR/ICCV)  
**Timeline**: 230 days @ 8h/day GPU (assumes professor loans laptop consistently)

### Phase 2-Alt: POC-6 Plan B (if POC-5.5 shows issues)
**Execute**: Add only MAE (#2) + MAML (#3), skip Damage Attn + Curriculum  
**Goal**: Solid results with lower risk  
**Deliverable**: Conference paper (CVPR/ICCV main track)  
**Timeline**: 215 days @ 8h/day GPU

---

## âš–ï¸ PLAN C FEASIBILITY: DEEP DIVE

### Can We Run Plan C on Dell Precision 7630 RTX 1000 Ada 6GB?

**Short Answer**: âœ… **YES, but with careful optimizations**.

**Critical Bottlenecks**:
1. **VRAM (6GB)**: Tight fit, requires batch_size=4, 384Ã—384 resolution, gradient checkpointing
2. **Training Time (1,841h)**: Requires ~7.7 months @ 8h/day access to Dell laptop
3. **Thermal Throttling**: 50W TDP laptop GPU may throttle after 4-6h continuous use

**Mitigation Strategies**:

#### VRAM Optimization
```python
# Enable gradient checkpointing (trade 30% speed for 50% VRAM)
model.encoder.set_grad_checkpointing(enable=True)

# Mixed precision with AMP
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

# Batch size 4 (vs 8 on larger GPUs)
train_loader = DataLoader(dataset, batch_size=4, ...)

# Resolution 384Ã—384 (vs 512Ã—512)
transform = A.Compose([A.Resize(384, 384), ...])
```

#### Training Time Optimization
```python
# Early stopping (aggressive)
early_stop = EarlyStopping(patience=10, min_delta=0.001)

# Reduce MAE pretraining epochs (50 â†’ 30)
mae_pretrain(encoder, epochs=30)  # Still effective per literature

# Reduce MAML meta-epochs (100 â†’ 60)
maml_train(model, tasks, meta_epochs=60)  # Faster convergence with good init
```

#### Thermal Management
- **Laptop cooling pad**: Keep GPU <75Â°C
- **Batch training**: 4h sessions with 30min cooldown (prevents throttling)
- **Undervolting**: -50mV to -100mV (reduces heat without performance loss)

**Revised Plan C Timeline** (with optimizations):
| Original | Optimized | Reduction |
|----------|-----------|-----------|
| MAE: 300h | MAE: 180h (30 epochs) | -120h |
| Multiclass: 1,140h | Multiclass: 1,050h (early stop) | -90h |
| MAML: 380h | MAML: 230h (60 epochs) | -150h |
| **Total: 1,841h** | **Total: 1,481h** | **-360h (-20%)** |

**Optimized Timeline**: 
- **Sequential** (1 GPU @ 8h/day): 1,481h Ã· 8h = **185 days â‰ˆ 6.2 months** (IMPRACTICAL)
- **Sequential** (1 GPU @ 24/7): 1,481h Ã· 24h = **62 days â‰ˆ 2 months** (laptop thermal risk)
- **Parallel** (cluster 8 GPUs @ 24/7): 1,481h Ã· 8 = **185h per GPU** â†’ **7.7 days â‰ˆ 1 week** ðŸš€

### Can We Run POC-5.5 on Current PC First?

**User's Current PC** (unknown specs, but likely weaker than Dell):
- Assume: GTX 1060/1650 or RTX 2060 (4-6GB VRAM)
- Goal: Run POC-5.5 (multiclass 30 epochs) to validate approach

**POC-5.5 Requirements**:
- VRAM: ~5GB (same as Plan C, batch_size=4)
- Time: 315h GPU (3 models Ã— 30 epochs Ã— 3.5h)
- Timeline: 315h Ã· 8h/day = **40 days** (realistic on current PC)

**Recommendation**: âœ… **YES, run POC-5.5 on current PC** as validation:
- If current PC has â‰¥4GB VRAM: Can run POC-5.5 (batch_size=2-4, 256-384px)
- If successful: Confidence to request Dell Precision 7630 for full Plan C
- If issues: Pivot to Plan B (fewer innovations) or merge to 8 classes

---

## â˜ï¸ CLUSTER OPTIONS: Game Changer for Timeline

### Problem with Single GPU (Dell Precision 7630)
- **Sequential training**: 1 model at a time = 1,481h GPU time = 2 months continuous 24/7
- **Thermal risk**: Laptop GPU running 24/7 for 2 months (high failure risk)
- **Occupancy**: Professor can't use laptop for 2 months
- **Impractical**: Even @ 8h/day = 6 months calendar time

### Solution: Cluster/Cloud GPUs (Parallelize Everything)

#### Option 1: University Cluster (FREE, best option if available)
**Typical Setup**:
- 10-20 GPUs (RTX 3090/4090, A100, V100)
- SLURM job scheduler
- Shared queue (may wait 1-3 days for allocation)

**Timeline with Cluster**:
| Phase | Sequential (1 GPU) | Parallel (8 GPUs) | Speedup |
|-------|-------------------|-------------------|---------|
| MAE Pretrain (3 encoders) | 180h | 180h Ã· 3 = **60h** (run 3 parallel jobs) | 3x |
| Multiclass (3 models) | 1,050h | 1,050h Ã· 3 = **350h** (run 3 parallel jobs) | 3x |
| MAML Meta-Learning | 230h | 230h (single job, can't parallelize inner loop) | 1x |
| MAML Adaptations (42) | 21h | 21h Ã· 8 = **3h** (run 8 parallel jobs) | 8x |
| **Total** | **1,481h** | **643h cluster time** | **2.3x** |

**Wall-clock time**: 643h Ã· 24h/day = **27 days @ 24/7** â†’ **~1 month** ðŸš€

**How to request**:
```bash
# Typical SLURM submission
sbatch --gres=gpu:1 --time=48:00:00 --job-name=mae_convnext train_mae.sh
sbatch --gres=gpu:1 --time=72:00:00 --job-name=multiclass_swin train_multiclass.sh
# etc. (submit all jobs, they run in parallel when GPUs available)
```

**Advantages**:
- âœ… FREE (university resource)
- âœ… Professional GPUs (A100 80GB >> RTX 1000 Ada 6GB)
- âœ… Can run 10+ jobs in parallel
- âœ… No thermal/laptop damage risk

**Disadvantages**:
- âš ï¸ Queue wait times (1-3 days to start, not immediate)
- âš ï¸ Job time limits (often 48-72h max per job, need checkpointing)
- âš ï¸ Shared resource (lower priority for undergrad/master students)

---

#### Option 2: Google Colab Pro/Pro+ (CHEAP, easy to start)
**Pricing**:
- **Colab Pro**: $9.99/month, T4/P100 GPU, 24h max runtime
- **Colab Pro+**: $49.99/month, V100/A100 GPU, longer runtimes, background execution

**Timeline with Colab Pro+**:
- Can run multiple notebooks in parallel (3-5 sessions)
- V100 ~2x faster than RTX 1000 Ada
- 1,481h on RTX 1000 â†’ 740h on V100 â†’ 740h Ã· 5 parallel = **148h wall-clock** â†’ **6 days** ðŸš€

**Cost**: $49.99/month Ã— 1 month = **~$50 total**

**Advantages**:
- âœ… Start immediately (no approval needed)
- âœ… Very cheap ($50 for entire POC-6)
- âœ… Can run 3-5 parallel notebooks
- âœ… Persistent storage (Google Drive)

**Disadvantages**:
- âš ï¸ 24h max runtime (need checkpointing every 24h)
- âš ï¸ May disconnect randomly (background execution helps)
- âš ï¸ Shared GPU pool (not guaranteed availability during peak hours)

---

#### Option 3: Kaggle Notebooks (FREE, 30h/week GPU quota)
**Specs**:
- FREE Tesla P100 or T4 GPU
- 30 hours/week GPU quota (resets weekly)
- 9h max session runtime

**Timeline with Kaggle**:
- 1,481h Ã· 30h/week = **50 weeks** (sequential, IMPRACTICAL)
- Run 2-3 parallel notebooks: 50 weeks Ã· 3 = **17 weeks â‰ˆ 4 months**

**Advantages**:
- âœ… Completely FREE
- âœ… Good for POC-5.5 validation (315h Ã· 30h/week = 11 weeks with 1 notebook)

**Disadvantages**:
- âŒ 30h/week limit too restrictive for POC-6 full
- âŒ 9h max session (frequent restarts)
- âš ï¸ Better for testing/validation, not full training

---

#### Option 4: AWS/Azure/GCP Cloud (EXPENSIVE, full control)
**Pricing** (AWS p3.2xlarge - V100 GPU):
- $3.06/hour on-demand
- $0.92/hour spot instance (can be terminated anytime)

**Timeline with 4Ã— Spot Instances**:
- 1,481h Ã· 4 parallel = 370h per GPU â†’ **370h wall-clock** â†’ **15.4 days**

**Cost**:
- **On-demand**: 1,481h Ã— $3.06 = **$4,532** (VERY EXPENSIVE)
- **Spot**: 1,481h Ã— $0.92 = **$1,363** (still expensive, but 70% cheaper)

**Advantages**:
- âœ… Full control, no queue
- âœ… Can provision 10+ GPUs instantly
- âœ… Professional infrastructure

**Disadvantages**:
- âŒ EXPENSIVE ($1,300+ even with spot instances)
- âŒ Spot can be terminated (need robust checkpointing)
- âš ï¸ Overkill for research project (unless funded grant)

---

#### Option 5: Paperspace Gradient (CHEAP cloud, good middle ground)
**Pricing**:
- RTX 4000 (8GB): $0.51/hour
- RTX 5000 (16GB): $0.78/hour
- A4000 (16GB): $0.76/hour

**Timeline with 4Ã— RTX 4000 (parallel)**:
- 1,481h Ã· 4 = 370h per GPU â†’ **370h wall-clock** â†’ **15.4 days**

**Cost**:
- 1,481h Ã— $0.51 = **$755** (much cheaper than AWS)

**Advantages**:
- âœ… Cheaper than AWS/Azure (50% less)
- âœ… Easy setup (Jupyter notebooks)
- âœ… Free tier available (M4000, limited hours)

**Disadvantages**:
- âš ï¸ Less reliable than AWS (smaller company)
- âš ï¸ Still ~$750 cost

---

### ðŸŽ¯ CLUSTER RECOMMENDATION

#### Best Options Ranked:

**ðŸ¥‡ 1st Choice: University Cluster (if available)**
- **Cost**: FREE âœ…
- **Timeline**: ~1 month wall-clock (27 days @ 24/7)
- **Action**: Ask your advisor: "Does the university have a GPU cluster for research? How do I request access?"
- **Likely scenarios**:
  - CS/Engineering department: Often have small cluster (5-10 GPUs)
  - National supercomputing center: Requires proposal (1-2 page research plan)

**ðŸ¥ˆ 2nd Choice: Google Colab Pro+ ($50/month)**
- **Cost**: ~$50 total âœ…âœ…
- **Timeline**: ~1 week wall-clock (6-7 days with 5 parallel notebooks)
- **Action**: Subscribe immediately, start POC-5.5 validation today
- **Best for**: Fast iteration, testing, POC-5.5 validation

**ðŸ¥‰ 3rd Choice: Paperspace Gradient ($755)**
- **Cost**: $755 (moderate) âš ï¸
- **Timeline**: ~15 days wall-clock (with 4 parallel GPUs)
- **Action**: If university cluster not available and need full control
- **Best for**: Serious research project, funded by advisor/grant

**âŒ Not Recommended: AWS/Azure ($1,300+)**
- Too expensive for student research
- Only if grant-funded or company-sponsored

**âŒ Not Recommended: Single Dell laptop (6 months)**
- Timeline too long (6 months @ 8h/day)
- Thermal risk (laptop not designed for 24/7 compute)
- Blocks professor's laptop for half a year

---

### ðŸ’¡ REVISED RECOMMENDATION: Hybrid Approach

```
Phase 0-1 (Week 1-2): Dataset Verification + Code Setup
â”œâ”€ Hardware: Your current PC (no GPU needed, CPU-only tasks)
â”œâ”€ Tasks: Download ARTeFACT, analyze class distribution, implement code
â”œâ”€ Cost: $0
â”œâ”€ Time: 2 weeks human work

Phase 2 (Week 3-4): POC-5.5 Validation  
â”œâ”€ Hardware: Google Colab Pro+ ($50/month) or University cluster
â”œâ”€ Tasks: Train 3 models (30 epochs each) to validate multiclass works
â”œâ”€ Cost: $50 (Colab) or $0 (cluster)
â”œâ”€ Time: 1 week wall-clock (3 parallel Colab notebooks)
â”œâ”€ Decision Point: GO/NO-GO for Plan C

Phase 3 (Week 5-8): POC-6 Plan C Full
â”œâ”€ Hardware: University cluster (best) or Colab Pro+ ($50Ã—2 months)
â”œâ”€ Tasks: MAE pretraining + Multiclass + MAML + all innovations
â”œâ”€ Cost: $0 (cluster) or $100 (Colab 2 months)
â”œâ”€ Time: 1 month wall-clock (cluster) or 2 weeks (Colab Pro+ with many parallel sessions)
â””â”€ Deliverable: Full results, paper-ready

Total Cost: $0-$150 (vs $755-$1,300 cloud options)
Total Time: 2 months wall-clock (vs 6 months on single laptop)
```

---

## ðŸš€ IMMEDIATE ACTION PLAN

### This Week: Validate Cluster Access

**Step 1: Email your advisor/professor** (TODAY)
```
Subject: GPU Cluster Access Request for Thesis Research

Profesor [Name],

Para mi tesis sobre detecciÃ³n de daÃ±os en arte patrimonial, necesito
entrenar modelos de deep learning (CNNs y Vision Transformers).

EstimaciÃ³n de recursos:
- ~1,500 GPU-hours totales
- 8-10 GPUs en paralelo â†’ 1 mes de tiempo real
- Compatible con RTX 3090, V100, A100 (6GB+ VRAM)

Â¿La universidad tiene acceso a un cluster de GPUs para investigaciÃ³n?
De ser asÃ­, Â¿cuÃ¡l es el proceso para solicitar acceso?

Alternativa: Estoy evaluando Google Colab Pro+ ($50/mes) si no hay 
cluster disponible.

Gracias,
[Your name]
```

**Step 2: While waiting for response, start with FREE options**
```bash
# Today: Try Kaggle (FREE 30h/week)
# Sign up: https://www.kaggle.com
# Upload POC-5 code, test training 1 model for 1 epoch
# Verify VRAM usage, training speed

# Tomorrow: Try Google Colab FREE tier
# Upload code to Google Colab
# Train for 1 epoch, measure time
# If works well â†’ upgrade to Colab Pro+ ($10/month for testing)
```

**Step 3: Download dataset NOW (CPU task, run on your PC)**
```bash
cd experiments/artefact-data-obtention
huggingface-cli download danielaivanova/damaged-media --repo-type dataset
python scripts/analyze_dataset.py  # Count samples, class distribution
```

---

## ðŸ“Š FINAL TIMELINE COMPARISON

| Approach | Hardware | Wall-Clock Time | Cost | Feasibility |
|----------|----------|-----------------|------|-------------|
| **Sequential (1 Dell laptop @ 8h/day)** | RTX 1000 Ada 6GB | **6 months** | $0 | âŒ IMPRACTICAL |
| **Sequential (1 Dell laptop @ 24/7)** | RTX 1000 Ada 6GB | **2 months** | $0 | âš ï¸ RISKY (thermal) |
| **University Cluster (8 GPUs)** | A100/V100 mix | **1 month** | $0 | âœ…âœ…âœ… BEST |
| **Google Colab Pro+ (5 parallel)** | V100/A100 | **1-2 weeks** | $50-100 | âœ…âœ… EXCELLENT |
| **Kaggle (3 parallel notebooks)** | P100/T4 | **4 months** | $0 | âš ï¸ Slow but FREE |
| **Paperspace (4 GPUs)** | RTX 4000/5000 | **15 days** | $755 | âš ï¸ Expensive |
| **AWS Spot (4 GPUs)** | V100 | **15 days** | $1,363 | âŒ Too expensive |

**Recommendation**: 
1. **Try to get university cluster** (email advisor TODAY) â†’ 1 month, $0
2. **Fallback: Colab Pro+** ($50-100) â†’ 1-2 weeks
3. **For testing NOW: Kaggle FREE** â†’ validate code works

---

## ðŸ“‹ FINAL RECOMMENDATION

## ðŸ“‹ FINAL RECOMMENDATION (UPDATED: Cluster-First Approach)

### **ANSWER TO YOUR QUESTION**:

> **"Â¿Estamos hablando de 6 semanas de entrenamiento seguidos?"**

âŒ **NO** - Era confusiÃ³n de cÃ¡lculo. Las opciones son:

1. **1 GPU (Dell laptop) secuencial**: 1,481 GPU-hours
   - @ 8h/dÃ­a: 185 dÃ­as = **6 meses calendario** âŒ IMPRACTICAL
   - @ 24/7: 62 dÃ­as = **2 meses continuos** âš ï¸ Risky (thermal, ocupa laptop del profesor)

2. **8 GPUs (cluster) paralelo**: 1,481 GPU-hours Ã· 8 GPUs
   - = 185h per GPU @ 24/7 = **7.7 dÃ­as = 1 semana** âœ…âœ…âœ…
   - Costo: **$0** (cluster universitario)

3. **5 GPUs (Colab Pro+) paralelo**: 1,481h Ã· 5 parallel sessions
   - = 296h wall-clock = **12 dÃ­as @ 24/7** âœ…âœ…
   - Costo: **$50-100** (1-2 meses Colab Pro+)

> **"Â¿Mejor veo si puedo pedir un cluster?"**

âœ… **SÃ, ABSOLUTAMENTE**. Cluster cambia todo:
- De **6 meses** (1 laptop) â†’ **1 semana** (cluster)
- De **impractical** â†’ **totally feasible**

---

### Phased Execution Plan (CLUSTER-OPTIMIZED)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TOTAL TIMELINE: 6-8 weeks wall-clock (NOT 6 months!)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Week 1-2: Setup (Your PC, no GPU needed)                        â”‚
â”‚   - Email advisor for cluster access                             â”‚
â”‚   - Download ARTeFACT dataset (CPU task)                         â”‚
â”‚   - Implement Hierarchical UPerNet code                          â”‚
â”‚   - Test 1 epoch on Kaggle FREE (validate code)                 â”‚
â”‚   - Deliverable: Code ready to deploy                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Week 3-4: POC-5.5 Validation (Cluster or Colab Pro+)            â”‚
â”‚   - Train 3 models (30 epochs each) in PARALLEL                  â”‚
â”‚   - Wall-clock: 4-7 days with cluster, 1 week with Colab        â”‚
â”‚   - Cost: $0 (cluster) or $50 (Colab Pro+)                      â”‚
â”‚   - Deliverable: GO/NO-GO decision for Plan C                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Week 5-8: POC-6 Plan C Full (Cluster REQUIRED)                  â”‚
â”‚   - MAE pretrain (3 encoders parallel): 2-3 days                â”‚
â”‚   - Multiclass training (3 models parallel): 10-14 days         â”‚
â”‚   - MAML meta-learning: 7-10 days                               â”‚
â”‚   - Wall-clock: 3-4 weeks with cluster                          â”‚
â”‚   - Cost: $0 (cluster) or $100-150 (Colab Pro+ 2-3 months)      â”‚
â”‚   - Deliverable: Full results, paper draft                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Changes from Previous Plan**:
- âŒ **OLD**: "6 months on single laptop @ 8h/day" â†’ IMPRACTICAL
- âœ… **NEW**: "6-8 weeks on cluster" â†’ TOTALLY FEASIBLE
- ðŸ’° **Cost**: $0 (cluster) or $50-150 (Colab fallback)

---

### IMMEDIATE NEXT STEPS (This Week)

#### âš¡ Priority 1: Cluster Access (TODAY)
```bash
# Email your advisor with this template:
```
**Subject**: Solicitud de acceso a cluster GPU para tesis (Heritage Art Damage Detection)

Estimado Profesor [Name],

Para mi trabajo de tesis sobre detecciÃ³n automÃ¡tica de daÃ±os en arte patrimonial, 
necesito entrenar modelos de deep learning (CNNs, Vision Transformers) en el 
dataset ARTeFACT (~445 imÃ¡genes anotadas, 16 clases).

**Requerimientos estimados**:
- ~1,500 GPU-hours totales (~2 meses en 1 GPU, pero **1 semana en cluster con 8 GPUs**)
- Compatible con: RTX 3090/4090, V100, A100 (mÃ­nimo 6GB VRAM)
- Framework: PyTorch + Docker
- Prioridad: Media (tesis de pregrado/maestrÃ­a, deadline paper Nov 2025)

**Preguntas**:
1. Â¿La universidad/departamento tiene un cluster de GPUs disponible?
2. Â¿CuÃ¡l es el proceso de solicitud? Â¿Necesito propuesta escrita?
3. Â¿Hay cola de espera? Â¿Tiempo lÃ­mite por job?

**Plan B alternativo**:
Si no hay cluster disponible, usarÃ© Google Colab Pro+ ($50/mes, ~1-2 semanas).

Adjunto: Resultados POC-5 (binary segmentation, 71% mIoU con MaxViT-Tiny).

Gracias por su apoyo,
[Your Name]
```

#### âš¡ Priority 2: Test FREE Options (While Waiting for Response)

**Option A: Kaggle (30h/week FREE)**
```bash
# 1. Create account: https://www.kaggle.com
# 2. Upload your POC-5 code as Kaggle notebook
# 3. Run 1 epoch training to measure speed:
#    - If 1 epoch = 3.5h on RTX 1000 Ada
#    - On Kaggle P100: expect ~2h per epoch (faster)
# 4. Validates: Code works, VRAM fits, training stable
```

**Option B: Google Colab FREE Tier**
```bash
# 1. Upload POC-5 code to Google Colab
# 2. Connect to T4 GPU (free tier)
# 3. Run 1 epoch training
# 4. If works â†’ upgrade to Pro+ ($9.99/month for testing)
```

#### âš¡ Priority 3: Download Dataset (CPU Task, Run Today)
```bash
cd /home/brandontrigueros/DevWSL/HeritageArt-CNN-ViT-Hybrid/experiments/artefact-data-obtention

# Download full ARTeFACT
huggingface-cli download danielaivanova/damaged-media --repo-type dataset --local-dir ./data/full

# Count samples and analyze
python -c "
import datasets
ds = datasets.load_dataset('danielaivanova/damaged-media')
print(f'Total samples: {len(ds[\"train\"])}')
print(f'Columns: {ds[\"train\"].column_names}')
print(f'First sample: {ds[\"train\"][0]}')
"
```

---

### Decision Matrix: Cluster vs Colab vs Dell Laptop

| Factor | University Cluster | Google Colab Pro+ | Dell Laptop (24/7) |
|--------|-------------------|-------------------|-------------------|
| **Wall-clock time** | ðŸŸ¢ **1 week** | ðŸŸ¢ **1-2 weeks** | ðŸ”´ 2 months |
| **Cost** | ðŸŸ¢ **$0** | ðŸŸ¡ $50-150 | ðŸŸ¢ $0 |
| **Reliability** | ðŸŸ¢ High (SLURM queue) | ðŸŸ¡ Medium (may disconnect) | ðŸŸ¡ Medium (thermal risk) |
| **Ease of setup** | ðŸŸ¡ Requires approval | ðŸŸ¢ Instant (credit card) | ðŸŸ¢ Already have laptop |
| **Parallel jobs** | ðŸŸ¢ 8-12 GPUs | ðŸŸ¡ 3-5 sessions | ðŸ”´ 1 GPU only |
| **Availability** | ðŸŸ¡ Unknown (ask advisor) | ðŸŸ¢ Guaranteed | ðŸŸ¢ Guaranteed |
| **Professional GPUs** | ðŸŸ¢ A100/V100 (fast) | ðŸŸ¢ V100/A100 | ðŸ”´ RTX 1000 Ada (slow) |
| **Thermal safety** | ðŸŸ¢ Datacenter cooling | ðŸŸ¢ Cloud (N/A) | ðŸ”´ Laptop overheating risk |
| **Professor impact** | ðŸŸ¢ Zero (shared resource) | ðŸŸ¢ Zero | ðŸ”´ Blocks laptop 2 months |

**Recommendation**:
1. **Try cluster first** (email advisor TODAY) â†’ Best option (free + fast)
2. **If cluster not available or >2 week queue**: Use Colab Pro+ ($50-150)
3. **Avoid Dell laptop 24/7**: Only use for testing/validation (not full training)

---

### âœ… YOUR ANSWER: "Can I Run Plan C?"

**Short Answer**: âœ… **YES, if you get cluster or Colab Pro+**

**Timeline Comparison**:

| Scenario | Wall-Clock Time | Cost | Feasibility |
|----------|----------------|------|-------------|
| âŒ **1 Dell laptop @ 8h/day** | 6 months | $0 | IMPRACTICAL |
| âš ï¸ **1 Dell laptop @ 24/7** | 2 months | $0 | RISKY (thermal) |
| âœ… **University cluster** | **1 week** | **$0** | **IDEAL** |
| âœ… **Colab Pro+ (5 parallel)** | **1-2 weeks** | **$50-150** | **EXCELLENT** |

**My Recommendation**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸŽ¯ RECOMMENDED PATH: Cluster-First Hybrid         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Email advisor TODAY (request cluster access)  â”‚
â”‚ 2. While waiting: Download dataset (CPU task)    â”‚
â”‚ 3. Test code on Kaggle FREE (validate it works)  â”‚
â”‚ 4. If cluster approved (1-2 weeks):              â”‚
â”‚    â†’ Run POC-5.5 + POC-6 on cluster (6-8 weeks)  â”‚
â”‚ 5. If cluster NOT available:                     â”‚
â”‚    â†’ Use Colab Pro+ ($50-150, 3-4 weeks total)   â”‚
â”‚ 6. Dell laptop: Only for testing/debugging       â”‚
â”‚    â†’ NOT for full training runs                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Expected Total Time**: 
- **Best case** (cluster): 6-8 weeks wall-clock, $0
- **Fallback** (Colab Pro+): 4-6 weeks wall-clock, $100-150
- **Worst case** (Dell 24/7): 2-3 months, $0 (but risky)

---

### ðŸŽ¬ What We Do RIGHT NOW

**I need you to confirm**:

1. âœ… **I will email my advisor TODAY** to ask about cluster access
2. âœ… **I will download ARTeFACT dataset** this week (CPU task, can run on your PC)
3. âœ… **I will test code on Kaggle FREE** (30h/week) to validate it works
4. **Choose fallback option** if cluster not available:
   - ðŸŸ¢ Option A: Colab Pro+ ($50-150, fastest fallback, 1-2 weeks)
   - ðŸŸ¡ Option B: Kaggle only (FREE but slow, 3-4 months)
   - ðŸ”´ Option C: Dell laptop 24/7 (FREE but risky, 2 months)

**Once you confirm, I will**:
- Help you write the cluster request email (tailored to your university)
- Create POC-5.5 code structure (hierarchical UPerNet implementation)
- Setup Kaggle/Colab notebooks for testing
- Download and analyze ARTeFACT dataset

**Your decision**: Â¿Procedemos con el plan Cluster-First? (Email advisor + test on Kaggle while waiting)

---
