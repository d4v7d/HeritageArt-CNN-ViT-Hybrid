\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
% \usepackage{minted} % Syntax highlighting
\usepackage{hyperref} % Cross-references are links
\usepackage{tabularx} % Add x word-wrap in table cells
\usepackage{listings} % Syntax highlighting
\usepackage[svgnames]{xcolor} % Listing background
\usepackage{flafter} % Figures after its reference in text
\usepackage[all]{nowidow} % Avoid widow lines
\usepackage{booktabs} % Horizontal lines in tables
\usepackage{verbatim} % Multi-line comments
\usepackage{booktabs,tabularx,makecell,array,xcolor,adjustbox}
\usepackage{url}
\usepackage[colorinlistoftodos]{todonotes}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{inconsolata} % Monospaced font

% Listing styles, see https://www.overleaf.com/learn/latex/Code_listing
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.95}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{red},
  showstringspaces=false
}

% See https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings#Settings
\lstdefinestyle{listing_style}{
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\footnotesize, % font family and size
    breakatwhitespace=true, % automatic breaks only at whitespace
    breaklines=true, % automatic line-breaking
    captionpos=b, % t=top, b=bottom
%   commentstyle=\color{codegreen},
    escapeinside={~[}{]~}, % Delimiters for Latex commands, eg: ~[\textbf{make test}]~
    frame=single, % none/leftline/topline/bottomline/lines/single/shadowbox
    keepspaces=true,
%   keywordstyle=\color{blue},
%   morekeywords={make,test,clean,asan,mkdir},
    numbers=left,
    numbersep=5pt, % distance of line-numbers from the code
    numberstyle=\tiny\color{codegray},
    rulecolor=\color{codegray}, % colour of the frame-box
%   showspaces=false,
%   showstringspaces=false,
%   showtabs=false,
%   stringstyle=\color{codepurple},
    tabsize=2
}

\lstset{style=listing_style}

% Adapted from https://tex.stackexchange.com/a/279134
\makeatletter
\def\ps@IEEEtitlepagestyle{%
  \def\@oddfoot{\mycopyrightnotice}%
  \def\@evenfoot{}%
}
\def\mycopyrightnotice{%
  {\footnotesize 979-8-3503-1887-6/23/\$31.00 \copyright 2023 IEEE\hfill}
}
\makeatother

% Embed fonts for IEEE Pdf Express
% https://www.overleaf.com/learn/latex/Questions/My_submission_was_rejected_by_the_journal_because_%22Font_XYZ_is_not_embedded%22._What_can_I_do%3F

\graphicspath{{figures/}}

\begin{document}

% Detection of Conservation-Relevant Features in Cultural Heritage Artworks Using Vision Transformers and CNNs
\title{Detection of Conservation-Relevant Features in Cultural Heritage Artworks Using Vision Transformers and CNNs}

\newif\ifanonymous
\anonymoustrue % Enable this line for blind review

\newcommand{\AnonymousAuthor}{%
\IEEEauthorblockN{Anonymous Name Last-Names}
\IEEEauthorblockA{\textit{Department} \\
\textit{Affiliation}\\
Address \\
email@address}
}

\author{
\ifanonymous

\AnonymousAuthor\and
\AnonymousAuthor\and
\AnonymousAuthor\and
\AnonymousAuthor\and
\AnonymousAuthor\and
\AnonymousAuthor

\else

\IEEEauthorblockN{Brandon Trigueros-Lara}
\IEEEauthorblockA{\textit{ECCI} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
brandon.trigueros@ucr.ac.cr}
\and
\IEEEauthorblockN{Valentino Vidaurre-Rodríguez}
\IEEEauthorblockA{\textit{ECCI} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
valentino.vidaurre@ucr.ac.cr}
\and
\IEEEauthorblockN{David González-Villanueva}
\IEEEauthorblockA{\textit{ECCI} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
david.gonzalezvillanueva@ucr.ac.cr}
\and
\IEEEauthorblockN{Rubén González-Villanueva}
\IEEEauthorblockA{\textit{ECCI} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
ruben.gonzalezvillanueva@ucr.ac.cr}
\and
\IEEEauthorblockN{Christian Quesada-López}
\IEEEauthorblockA{\textit{ECCI} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
cristian.quesadalopez@ucr.ac.cr}
\and
\IEEEauthorblockN{Jeisson Hidalgo-Céspedes}
\IEEEauthorblockA{\textit{ECCI-CITIC} \\
\textit{Universidad de Costa Rica}\\
San José, Costa Rica \\
jeisson.hidalgo@ucr.ac.cr}
\fi
}

\maketitle

\begin{abstract}
Preserving the integrity and cultural authenticity of artworks is a fundamental challenge in heritage conservation. Traditionally, conservators rely on visual inspection and scientific imaging techniques such as X-ray, infrared photography, and microscopy. However, despite significant advances in computer vision, its application to cultural heritage remains scarce and predominantly focused on digital restoration rather than preventive conservation.

Building on the ARTeFACT benchmark by Ivanova et al., we adopt its multiclass damage taxonomy and evaluation protocol to compare three architecture families for semantic damage segmentation: Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) and Hybrid CNN-ViTs. 

Our study has two goals: first, to reproduce and validate strong baselines reported on ARTeFACT under a unified implementation; and second, to probe cross-dataset transferability—training on ARTeFACT and evaluating out-of-distribution performance on additional public collections—to assess robustness under domain shift. We report region-level \emph{m}IoU, Dice/F1, and per-class IoU, and analyze error modes to characterize when each family is preferable (e.g., high-frequency versus context-dependent degradations). 

The results establish reproducible baselines and illuminate the comparative behavior of CNNs, Transformers, and hybrids for artwork damage segmentation. We hypothesize that CNNs and ViTs will exhibit complementary strengths. CNNs outperform on fine, high-frequency degradations (e.g., hairline cracks), whereas ViTs excel on broader, context-dependent phenomena (e.g., varnish degradation/over-painting). We evaluate agreement with expert annotations and will release fixed data splits and baselines, aiming to offer actionable guidance for AI-assisted, region-level analysis in conservation workflows.
\end{abstract}

\begin{IEEEkeywords}
Computer Vision; Cultural Heritage Preservation; Digital Heritage; Artwork Deterioration Detection; Convolutional Neural Networks; Vision Transformers
\end{IEEEkeywords}


\section{Introduction}
\label{sec:Introduction}

Artwork conservation is a core aspect of cultural heritage stewardship. Socially, it underpins social identity, community pride, and education~\cite{mishra_artificial_2024}, while also contributing to tourism and the art market, generating jobs and supporting economies~\cite{ernst__young_global_limited_2014_creating_2014}. Conservation helps preserve the historical, aesthetic, and ethical value of artworks for future generations~\cite{ernst__young_global_limited_2014_creating_2014}.

The field of conservation can be broadly sliced to three branches: preventive conservation, remedial conservation, and restoration~\cite{icom-cc_icom-cc_2009}. We focus on \textbf{preventive conservation}, i.e. managing environments and risks to slow or avoid deterioration before damage occurs~\cite{icom-cc_icom-cc_2009}. Since damage must be identified before being addressed, our analysis could also assist restoration workflows.

Timely recognition of surface deterioration (such as abrasion, pigment loss, craquelure, varnish degradation, or water staining) is key~\cite{ivanova_artefact_2024}. Conservators traditionally use visual inspection and scientific imaging methods, including X-ray, infrared reflectography, microscopy, and spectroscopy~\cite{borg_application_2020}. Early computer-assisted tools included brushstroke extraction~\cite{lamberti_computer-assisted_2014}, crack and craquelure analysis~\cite{sidorov_craquelure_2019, dulecha_crack_2019}, and material-layer separation~\cite{pu_cross-domain_2022}. However, these were generally developed as case-specific tools with limited generalization and no standardized benchmarks.

During the 2010s, \textbf{Convolutional Neural Networks} (CNNs) arose as an architecture of deep learning models composed of multiple layers of convolution filters and pooling operations that progressively extract higher-level representations~\cite{mauricio_comparing_2023}. CNN's became the default machine learning (ML) tool for image classification, segmentation and pattern recognition, due to their ability to learn spatial hierarchies of features at multiple scales~\cite{yunusa_exploring_2024}.

In 2020, the \textbf{Vision Transformers} (ViTs)~\cite{dosovitskiy_image_2021} architecture extended the Transformer architecture from natural language processing to vision tasks, enabling modeling of global context and long-range dependencies in a single layer~\cite{mauricio_comparing_2023}.ViTs often match or exceed CNN performance but typically demand larger datasets and higher compute resources~\cite{ali_vision_2023,elharrouss_vits_2025,yunusa_exploring_2024,dosovitskiy_image_2021}, which are important considerations when adopting them for niche domains like cultural heritage imaging.

Since 2019, AI applications to cultural heritage have accelerated, spanning damage detection~\cite{kwon_automatic_2019, samhouri_prediction_2022, hou_using_2024}, materials analysis~\cite{go_comparison_2025}, accessibility~\cite{girbacia_analysis_2024, mishra_artificial_2024}, and image reconstruction~\cite{yi_zhang_artificial_2025}. These models automate visual inspection and support scalable, less subjective analysis compared to manual methods~\cite{sankar_transforming_2023, yunusa_exploring_2024}.

Yet, most studies use either CNNs or ViT s in isolation, rather than benchmarking them directly~\cite{ali_vision_2023, roy_multimodal_2023}. A notable exception is the 2024 \textbf{ARTeFACT benchmark}~\cite{ivanova_artefact_2024}, which introduced a large annotated dataset and tested several CNN and ViT models for multi-type damage segmentation. This was accompanied by dataset documentation efforts~\cite{alkemade_datasheets_2023}. Another recent study compared CNNs and ViTs for pigment classification~\cite{go_comparison_2025}, but not damage detection.

Progress is also hindered by data scarcity. Licensing restrictions and the cost of expert annotations~\cite{ivanova_artefact_2024}, combined with institutional differences in materials, imaging, and damage taxonomies, create cross-domain variability that affects model training and results in poor generalization when moving across domains. Consequently, there is strong interest in metadata automation~\cite{diem_automatic_2023} and transferable models.

We aim to address this by comparing three families of vision architectures (CNNs, ViTs, and hybrids) for damage detection in digitized artworks. Our research question is:

\begin{itemize}
    \setlength{\itemindent}{0.5cm}
    \item[RQ] Which Deep Learning architecture (CNN, Vision Transformers, or hybrid CNN–ViT) achieves the best multi-category paint damage segmentation?
    % \item[H1] With limited training data, CNNs will outperform ViTs on painting-deterioration localization; with strong pretraining or distillation, ViTs will match or surpass CNNs, and hybrid CNN–ViT models will achieve the best overall performance by combining local inductive bias with global attention.
\end{itemize}

We focus on damage segmentation in 2D artworks using RGB. While deployment in Costa Rican heritage collections is our long-term goal, we leave it to future work pending dataset availability.

\section{Previous Work}
\label{sec:PreviousWork}

As computer vision has advanced, its intersection with cultural heritage has grown, alongside a rapidly expanding body of literature. To situate our contribution within this landscape, we surveyed literature from 2013–2025 on computer vision for cultural heritage, focusing on image-based deterioration detection and on comparisons of Convolutional Neural Networks (CNNs) versus Vision Transformers (ViTs). We queried Scopus and arXiv (using keywords like “CNN”, “ViT”, “cultural heritage”, “damage detection”) and screened for peer-reviewed studies most relevant to our topic. 
\medskip
% Begin table
% Colors for strengths/deficiencies
\definecolor{Good}{RGB}{25,25,112}    % MidnightBlue
\definecolor{Bad}{RGB}{128,0,0}       % Maroon

% Helpers
\newcommand{\good}[1]{{\color{Good}#1}}
\newcommand{\bad}[1]{{\color{Bad}#1}}

% Wrapping column type for TabularX
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

% ---- Table ----

% \begin{table*}[h]
% \centering
% \caption{Key related works on artwork deterioration detection.}
% \footnotesize
% \setlength{\tabcolsep}{3pt}
% \renewcommand{\arraystretch}{1.2}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabularx}{\linewidth}{
%     >{\raggedright\arraybackslash}p{2.3cm}  % Citation
%     >{\raggedright\arraybackslash}p{2.2cm}  % Domain
%     >{\raggedright\arraybackslash}p{2cm}    % Task
%     >{\raggedright\arraybackslash}p{3.2cm}  % Model
%     >{\raggedright\arraybackslash}p{3.2cm}  % Output
%     >{\raggedright\arraybackslash}p{3.2cm}  % Notes
% }
% \toprule
% \textbf{Citation} & \textbf{Domain} & \textbf{Task} & \textbf{Model(s)} & \textbf{Output} & \textbf{Notes} \\
% \midrule

% \textit{Kwon \& Yu, 2019} & Stone monuments & Detection (multi-label) & \good{Faster R-CNN (4 damage types)} & Bounding boxes w/ class labels & High in-site accuracy (94\%); \bad{site-specific only, no x-domain test} \\

% \textit{García-Moreno et al., 2024} & Easel paintings & Segmentation & \good{Mask R-CNN (ARTDET)} & Binary masks for paint loss + fills & Pixel-level F1/recall~80\%; 2-class only; useful restoration tool \\

% \textit{Ivanova et al., 2024 (ARTeFACT)} & Mixed media artworks & Binary + multiclass segmentation & \good{UPerNet+ConvNeXt, Swin, SAM, DiffEdit, DINOv2} & Masks (15 damage types) + content/material tags & Benchmark dataset + splits; \bad{F1~0.5–0.6}; \good{code+data released} \\

% \textit{Amiri Darban et al., 2025} & Solar infrastructure & Multi-class anomaly detection & \good{Hybrid CNN–ViT (parallel)} & BBoxes + pixel segmentation & \good{Strong transfer, fusion helps generalization}; \bad{not CH-related} \\

% \textbf{This work} & Cultural Heritage (artworks) & Damage segmentation; cross-domain eval & \good{CNN (ConvNeXt); ViT (Swin); Hybrid CNN–ViT (new)} & Region-level pixel masks & Benchmarking \textit{transferability} + hybrid design; \good{Costa Rica = future work} \\

% \bottomrule
% \end{tabularx}
% \end{adjustbox}
% \label{tab:prevwork}
% \end{table*}

Early works applied classical image processing to assist restoration (e.g. crack detection and virtual inpainting on paintings). With the rise of deep learning, CNN-based detectors became prevalent for heritage surfaces. For example, Kwon and Yu (2019) trained a Faster R-CNN to automatically label cracks, losses, and biological growth on stone relics, achieving high confidence (94\%+ on test images)~\cite{kwon_automatic_2019}. Hatır et al. (2021) similarly used Mask R-CNN to segment seven weathering forms on a Hittite rock sanctuary, reporting mAP up to 89–100\% per class in training~\cite{hatir_deep_2021}. These case studies – often on historic fa\c{c}ades~\cite{wang_automatic_2024, samhouri_prediction_2022}, monuments~\cite{kwon_automatic_2019, hatir_deep_2021}, murals~\cite{yi_zhang_artificial_2025} or decorative tiles~\cite{karimi_deep_2024, hu_integrating_2025} – demonstrate that deep CNNs can learn visible damage patterns (cracks, spills, detachments, bio-colonization) with high in-sample accuracy. However, they generally rely on domain-specific datasets and do not test generalization beyond the original site or material. A few studies targeted painted artworks: Angheluta and Chiroșca (2020) re-used high-resolution photogrammetry images to train a CNN that detects cracks, blisters, and paint losses on a polychrome wood icon, successfully highlighting deterioration regions via activation maps~\cite{angheluta_physical_2020}. Likewise, Garcia-Moreno et al. (2024) developed ARTDET, a Mask R-CNN–based tool to identify lacunae (paint layer loss) in easel paintings, achieving ~80\% recall for missing-paint and stucco areas~\cite{garcia-moreno_artdet_2024}. Other case studies have explored degradation classification in temple walls~\cite{huang_deep_2025}, multi spectral separation in panel paintings~\cite{pu_mixed_2022}, and wall painting pixel classification~\cite{dulecha_crack_2019}, among others~\cite{cornelis_crack_2013, sankar_transforming_2023}. These works confirmed the feasibility of automated damage mapping in art, but each was limited to one damage type or artwork, with no direct comparison between different model architectures.

Recently, researchers have begun assembling broader benchmarks. Ivanova et al. (2024) introduced ARTeFACT~\cite{ivanova_artefact_2024}, a first-of-its-kind dataset of 418 images with over 11,000 expert annotations for 15 types of deterioration (e.g. craquelure, peeling, staining) across diverse analogue media. Using ARTeFACT to evaluate state-of-the-art segmentation models, they found that both CNN-backbone networks (UPerNet with ConvNeXt) and Transformer-based networks (Swin-Unet, SegFormer) fall short of reliable performance. For instance, even framing the task as binary segmentation (damaged vs. clean), the best supervised models reached only moderate pixel-level accuracy (F1\textasciitilde0.5–0.6) and struggled to generalize across different materials and image content~\cite{ivanova_artefact_2024}. No single architecture (CNN or ViT) clearly outperformed the other in all cases, suggesting each has complementary strengths and limitations. 

Comparisons between CNNs, ViTs, and hybrids are also emerging across domains, including pigment imaging~\cite{go_comparison_2025}, portrait classification~\cite{diem_automatic_2023}, and artwork identification~\cite{wang_fusion_2025}. In fact, hybrid approaches are gaining attention---for example, a CNN--ViT fusion recently outperformed standalone models in anomaly detection for solar farms~\cite{darban_anomaly_2025} and for surface degradation in vineyards~\cite{leite_comparative_2024}---hinting that combining both feature types could benefit cultural heritage tasks as well.

Several surveys and scientometric reviews highlight this trend. Mishra and Louren\c{c}o (2024) reviewed AI applications across CH subdomains~\cite{mishra_artificial_2024}, Rathi et al. (2025) synthesized deep models for fine art classification~\cite{rathi_survey_2025}, and Yunusa et al. (2024) surveyed hybrid CNN--ViT models for vision tasks~\cite{yunusa_exploring_2024}, all noting limited deployment in deterioration detection.

In summary, prior work shows growing interest in AI-driven visual inspection for cultural heritage. Deep CNNs have achieved high accuracy in detecting certain damages under controlled settings, and ViTs are being explored in related classification tasks. However, there remains no comprehensive study comparing traditional methods, CNNs, ViTs, and hybrid models on a common problem in artwork deterioration. Most studies address a single domain or damage type, often with small datasets and ad hoc metrics, making their results hard to generalize. The difficulty of robust damage detection across different artworks and conditions persists. This gap motivates our work: we seek to benchmark a hybrid CNN-ViT on a multi-type deterioration segmentation task, thereby advancing towards generalizable and reproducible solutions.

\section{Methods}
\label{sec:Methodology}

\subsection{Overview} 
To answer our research question, we implemented a unified segmentation pipeline that evaluates the performance of three architecture families, a CNN (ConvNeXt), a Transformer (Swin), and a hybrid (MaxViT), on multi-class deterioration segmentation. We used the ARTeFACT dataset~\cite{ivanova_artefact_2024} for training and evaluation, with a main focus on bechmarking per-class segmentation performance in-domain.

\subsection{Dataset}
\paragraph*{ARTeFACT Benchmark}
We used the ARTeFACT dataset~\cite{ivanova_artefact_2024} as our primary training and test corpus. It includes 418 high-resolution images of artworks from diverse media and styles, annotated with 15 pixel-level damage classes plus a \emph{Clean} label. The dataset is stratified by material and content type, but presents class imbalance and heterogeneous damage scales: many images contain large clean areas with only small damaged regions, and certain damage categories have far fewer examples than others. We accounted for this in model training (via loss function) and evaluation (using macro-averaged metrics).

\paragraph*{Data Splits}
Following the original dataset's recommendation~\cite{ivanova_artefact_2024}, we created a stratified split such that the training set contains roughly 70\% of the images, the validation set 15\%, and the test set 15\%. This yielded approximately 290 train images, 60 val, 60 test (exact counts vary slightly by ensuring at least one example of each damage type in each set). The stratification tries to prevent a scenario where, for example, a particular material or damage type is completely missing from validation or test, which could unfairly skew results. No images from the same artwork or collection appear in both training and test. All model selection (hyperparameter tuning, early stopping) was done on the validation set, and final performance for the research question was reported on the held-out test set.

\paragraph*{Data Preprocessing}
All images were kept in RGB color and no significant color correction or filtering was applied beyond what the dataset provides. We resized or downsampled images only if needed to fit memory during patch processing, otherwise maintaining the original resolution for maximal detail. Prior to feeding to the models, image pixel values were normalized to the range expected by the pretrained backbones (for ImageNet-pretrained models, we subtracted the mean and divided by the standard deviation of ImageNet training data). We did not perform explicit contrast enhancement or denoising, as the provided scans are generally of good quality~\cite{ivanova_artefact_2024}. However, our data augmentation implicitly covered some variations in color and scale.

\subsection{Model Architecture and Training Pipeline}
To address our research questions, we implemented a semantic
segmentation pipeline with three different model architectures. All
three models share a similar encoder-decoder design: an encoder backbone (convolutional, transformer, or hybrid) extracts multi-scale features from the input image, and a decoder head produces a pixel-wise classification (damage type) mask. For a fair comparison, we used the same decoder framework for all models: U-Net.~\cite{ronneberger_u-net_2015} We chose U-Net for its proven effectiveness in dense prediction tasks and its ability to combine low-level and high-level features through skip connections. This ensures that differences in performance can be attributed mainly to the backbone architecture (CNN vs ViT vs hybrid), not the decoding mechanism. All models were trained at $384\times384$ pixel resolution to balance memory efficiency with spatial detail preservation. Below we detail each model variant:

\subsubsection{CNN Model – ConvNeXt-T + U-Net}
For the convolutional baseline, we used ConvNeXt-Tiny as the backbone 
encoder. ConvNeXt is a modernized CNN architecture with ViT-inspired design improvementss~\cite{liu_convnet_2022,wang_upernet_2023}.  It builds on the ResNet paradigm but incorporates transformer-inspired features such as larger kernel convolutions (7×7), depthwise convolutions, LayerNorm instead of BatchNorm, and GELU activation functions, resulting in a pure-CNN that achieves competitive performance with vision transformersy~\cite{liu_convnet_2022}. We selected ConvNeXt-Tiny (\textasciitilde33.1M parameters) to maintain a model size appropriate for our dataset scale while avoiding overfitting. The hierarchical feature maps extracted at multiple scales (1/4, 1/8, 1/16, 1/32 of input resolution) are fed to the U-Net decoder, which progressively upsamples and combines features through skip connections to produce the final segmentation.

\paragraph*{Pretraining}
We initialized ConvNeXt-Tiny with weights pretrained on ImageNet-1K. The U-Net decoder was randomly initialized and trained from scratch on the ARTeFACT dataset. This approach follows standard transfer learning practices where pretrained encoders provide strong visual representations that are then adapted to the target segmentation task.

\subsubsection{Transformer Model – SegFormer-B3 + U-Net} 
For the pure vision transformer approach, we used SegFormer-B3 (Mix Transformer B3, MiT-B3) as the backbone encoder~\cite{xie_segformer_2021}. SegFormer represents a hierarchical transformer specifically designed for segmentation tasks. Unlike standard ViTs that produce single-scale features, SegFormer's Mix Transformer encoder generates multi-scale feature maps at four different resolutions through progressive patch merging and efficient self-attention mechanisms~\cite{xie_segformer_2021}. The B3 variant (\textasciitilde45.0M parameters) was selected to maintain comparable model capacity with our CNN baseline while representing state-of-the-art transformer segmentation approaches. SegFormer's key innovation is its hierarchical structure that eliminates the need for positional encodings, making it more robust to resolution changes. We coupled the SegFormer encoder with the same U-Net decoder used for all models, ensuring architectural consistency in our comparison.

\paragraph{Pretraining}
The SegFormer-B3 encoder was initialized with weights pretrained on ImageNet-1K. As with the CNN model, the U-Net decoder was randomly initialized. The pretrained transformer backbone provides strong semantic feature extraction capabilities that are then fine-tuned on the ARTeFACT damage segmentation task.

\subsubsection{Hybrid Model – MaxVit-T + u-Net)}
As an example of a hybrid architecture that combines convolutional and transformer elements, we chose MaxViT-Tiny~\cite{tu_maxvit_2022} as our third model. MaxViT (Multi-Axis Vision Transformer) strategically integrates convolution blocks with multi-axis self-attention mechanisms to capture both local patterns and global context~\cite{tu_maxvit_2022}. Each MaxViT stage contains two types of blocks: (1) MBConv blocks that apply depthwise convolutions for local feature extraction with strong inductive biases, and (2) transformer blocks that perform both local window attention and grid  attention across the entire feature map for global receptive fields. This design aims to combine the strengths of CNNs (translation equivariance, local pattern recognition, computational efficiency) with the strengths of ViTs (long-range dependencies, flexible receptive fields, superior generalization). We selected the Tiny variant (31.0M parameters) to maintain comparable model capacity with our other architectures. As with the other models, MaxViT-Tiny serves as the encoder backbone paired with the U-Net decoder.

\paragraph{Pretraining}
The MaxViT-Tiny encoder was initialized with weights pretrained on ImageNet-1K. The U-Net decoder was randomly initialized and trained from scratch. This hybrid backbone provides a unique combination of local and global feature extraction that we hypothesize may be particularly effective for diverse damage patterns in heritage artworks.

\subsection*{Training Procedure}
To ensure fair comparison across architectures, all models followed an identical training pipeline with only architecture-specific hyperparameters adjusted. We trained on the augmented ARTeFACT dataset using an 80/20 train-validation split, processing images at 384×384 pixel resolution. This resolution balances computational efficiency with sufficient spatial detail for damage detection.

\paragraph{Data Augmentation}
Our augmentation strategy combined geometric and photometric transformations to improve model robustness. We applied random horizontal flips (p=0.5), random rotation (±15°), random scaling (0.9-1.1×), and elastic deformations. For photometric augmentation, we used random brightness and contrast adjustments (±10\%), random gamma correction (0.9-1.1), and Gaussian blur ($σ ∈ [0, 1.5]$). We intentionally avoided aggressive color jittering to preserve damage-specific chromatic signatures (e.g., brown stains, yellow fading). Training patches were sampled uniformly from the full-resolution 
images, exposing models to damage patterns at various scales and positions.

\paragraph{Optimization and Regularization}
We employed the AdamW optimizer~\cite{loshchilov_decoupled_2019} with a weight decay of 0.01 for all models. To account for different GPU memory requirements of each architecture, we adjusted batch sizes while maintaining effective learning rates through proportional scaling: ConvNeXt-Tiny used batch size 96 with learning rate 0.001, SegFormer-B3 used batch size 32 with learning rate 0.000333 (scaled by 32/96), and MaxViT-Tiny used batch size 48 with learning rate 0.0005 (scaled by 48/96). This ensures comparable 
gradient statistics across architectures. We used OneCycleLR scheduling with 
a maximum learning rate at 30% of training and cosine annealing afterward. 
All models trained for 50 epochs on an NVIDIA Tesla V100S PCIe (32GB VRAM).

\paragraph{Loss Function}
We addressed the segmentation task using the Dice Loss function, which is well-suited for handling class imbalance by directly optimizing the overlap between predicted and ground truth masks. The Dice coefficient for each class $c$ is given by:

$$\text{Dice}_c = \frac{2 \sum_{i} p_{i,c} g_{i,c}}{\sum_{i} p_{i,c} + \sum_{i} g_{i,c} + \epsilon}$$

where $p_{i,c}$ is the predicted probability of pixel $i$ belonging to class $c$, $g_{i,c}$ is the ground truth binary value, and $\epsilon$ is a smoothing term. The total loss is computed as the average of $1 - \text{Dice}_c$ across all classes. Although we explored class weighting strategies to further address imbalance, we found that the standard multiclass Dice Loss provided stable training and competitive performance without the need for complex weighting schemes.

\subsection*{Inference}
For validation and testing, we performed inference at the same 384×384 pixel resolution used during training. Images were resized to the target resolution, processed by the model to obtain class logits, and then upsampled if necessary (though in our pipeline, input and output resolutions were matched). The final per-pixel class prediction is determined by the argmax over the class logits. This approach ensures consistency between the training and inference phases and allows for efficient batch processing.

\subsection{Evaluation Metrics}
We evaluate model performance using multiple complementary metrics to assess 
both classification accuracy and spatial localization quality. Our primary 
metric is mean Intersection-over-Union (mIoU), computed as the macro-average 
across all 16 damage classes (including the \emph{Clean} background class). For 
each class $c$, IoU is defined as:

$$\text{IoU}_c = \frac{TP_c}{TP_c + FP_c + FN_c}$$

where $TP_c$ are true positive pixels, $FP_c$ are false positives, and 
$FN_c$ are false negatives. The mean IoU averages these per-class values, 
giving equal weight to rare and common categories. This macro-averaging 
approach attempts to prevent the abundant \emph{Clean} class from dominating the metric.

We additionally report per-class Precision, Recall, and F1-score to provide 
insight into type I and type II error rates for each damage category:

$$\text{Precision}_c = \frac{TP_c}{TP_c + FP_c}, \quad \text{Recall}_c = \frac{TP_c}{TP_c + FN_c}, \quad$$

$$\text{F1}_c = \frac{2 \cdot \text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}$$

These per-class metrics reveal which damage types each architecture handles 
well versus poorly. For computational efficiency analysis, we measure 
inference time (milliseconds per image) on the validation set, averaged over 
all images. This provides a practical measure of deployment feasibility. These metrics enable a quantitative comparison of architecture families, responding to our research question.

\section{Results}
\label{sec:Results}

To evaluate the comparative performance of the three families of architectures—CNN (UNet + ConvNeXt-T), Transformer (UNet + SegFormer-B3), and Hybrid (UNet + MaxVit-T)—we report pixel-level segmentation accuracy across the 15 ARTeFACT damage categories. Metrics include per-class F1, per-class IoU, and macro-averaged mIoU. Overall, the hybrid model achieves the highest mean IoU (0.376) and mean F1 (0.681), followed by the Transformer model (mIoU 0.356, F1 0.552) and finally the CNN baseline (mIoU 0.307, F1 0.515). Inference times are comparable across models (7.1–8.6 ms per image), with the CNN being marginally faster and the Transformer slightly slower.

\subsection*{Overall Performance}

\begin{table}[!t]
\caption{Overall segmentation performance for the three model families.}
\label{tab:overall_pref}
\centering
% Make the table a bit tighter
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.9}

\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Model & mIoU ↑ & Mean F1 ↑ & Inference (ms) ↓ \\
\midrule
CNN (UNet + ConvNeXt-T) & 0.307 & 0.515 & \textbf{7.10} \\
Transformer (UNet + SegFormer-B3) & 0.356 & 0.552 & 8.61 \\
Hybrid (UNet + MaxViT-T) & \textbf{0.376} & \textbf{0.681} & 8.58 \\
\bottomrule
\end{tabular}%
}
\end{table}


\subsection*{Per-Class Performance} The chart below (Fig.~\ref{fig:perclass_iou}) visualizes the per-class IoU performance, highlighting the strengths of the Hybrid architecture.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figures/per_class_iou_chart.png}
\caption{Per-class IoU performance comparison across CNN, Transformer, and Hybrid architectures. The Hybrid model (MaxViT) consistently outperforms others on structural damages like Cracks, Staining, and Writing, while maintaining high accuracy on core classes.}
\label{fig:perclass_iou}
\end{figure*}

\begin{table}[!t]
\centering
\caption{Per-class F1-score for CNN, Transformer, and Hybrid models. Best value per class in bold.}
\label{tab:perclass_f1}
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.9}

\begin{tabular}{lccc}
\toprule
Damage Type & CNN & Transformer & Hybrid \\
\midrule
Clean & 0.962 & 0.971 & \textbf{0.975} \\
Material loss & 0.828 & 0.888 & \textbf{0.904} \\
Peel & 0.679 & 0.748 & \textbf{0.779} \\
Cracks & 0.185 & 0.432 & \textbf{0.447} \\
Structural defects & 0.494 & 0.656 & \textbf{0.682} \\
Dirt & 0.399 & 0.260 & \textbf{0.408} \\
Staining & 0.243 & 0.590 & \textbf{0.683} \\
Discolouration & 0.510 & 0.665 & \textbf{0.688} \\
Scratches & 0.344 & 0.568 & \textbf{0.631} \\
Burn marks & -- & \textbf{0.319} & -- \\
Hair & -- & -- & -- \\
Dust & -- & -- & -- \\
Lightleak & 0.379 & 0.799 & \textbf{0.729} \\
Stamp & -- & -- & \textbf{0.685} \\
Sticker & \textbf{0.769} & 0.643 & 0.663 \\
Puncture & 0.793 & \textbf{0.823} & 0.699 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Model Trends}

Across the three architectures evaluated, we observe clear differences in how each model family handles the heterogeneity of deterioration patterns present in the ARTeFACT dataset~\cite{ivanova_artefact_2024}. The \textbf{CNN model (UNet + ConvNeXt-T)} performs reliably on large, high-contrast categories such as \emph{Clean} (IoU 0.928) and \emph{Material loss} (0.707), and maintains reasonable performance on \emph{Peel} (0.520). However, its ability to capture more complex or elongated structures is limited. IoU drops sharply for classes such as \emph{Cracks} (0.102), \emph{Fold} (0.328), \emph{Writing} (0.249), and \emph{Staining} (0.138), and it essentially fails on small particulate and very sparse categories such as \emph{Dust} and \emph{Hair} (IoU = 0). Rare but visually salient categories are captured unevenly: the model attains strong scores on \emph{Sticker} (0.625) and \emph{Puncture} (0.690), but completely misses Stamp and Burn marks. Overall, the CNN is competitive when damage covers a relatively compact area with strong local contrast, but struggles with thin, fragmented, or low-contrast deterioration. These failure modes suggest that purely convolutional architectures struggle to capture extremely thin, sparse, or texture-subtle structures, particularly when they appear at irregular orientations or scales.

Replacing the ConvNeXt encoder with the \textbf{Transformer model (UNet + SegFormer-B3)} cyields consistent gains across most damage types and improves both mean IoU and mean F1. The Transformer model strengthens performance on the core damage classes: \emph{Clean} (IoU 0.945), \emph{Material loss} (0.777), and \emph{Peel} (0.611). It also markedly improves structured or textured categories such as \emph{Cracks} (0.285), \emph{Dirt} (0.149), \emph{Fold} (0.489), \emph{Writing} (0.506), and \emph{Staining} (0.419). For rare classes, the Transformer is often the only model with non-trivial IoU, particularly for \emph{Burn marks} (0.193) and \emph{Lightleak} (0.667), and it achieves the best IoU on \emph{Puncture} (0.698). Nevertheless, it still completely fails on \emph{Dust} and \emph{Hair}, mirroring the difficulties seen in the CNN. These trends suggest that multi-scale attention improves the recovery of mid-scale and rare structural damage, but does not fully solve the detection of extremely small artefacts.

The \textbf{Hybrid model (UNet + MaxVit-T)} provides the best overall trade-off between accuracy and efficiency, and dominates for most of the “core” ARTeFACT damage categories. It reaches the highest IoUs for \emph{Clean} (0.950), \emph{Material loss} (0.819), and \emph{Peel} (0.653), and further improves upon the Transformer on structurally complex damage types. In particular, the hybrid model attains the best IoUs for \emph{Scratch} (0.236), \emph{Dirt} (0.257), \emph{Fold} (0.504), \emph{Writing} (0.532), \emph{Cracks} (0.290), and \emph{Staining} (0.525). It is also the strongest model on \emph{Stamp} (0.606) and achieves competitive performance on \emph{Lightleak} (0.637), although the Transformer remains slightly better on the latter. As with the other families, \emph{Dust} and \emph{Hair} remain undetected (IoU = 0). Altogether, these results indicate that combining convolutional inductive biases with MaxViT’s multi-scale attention yields the most robust segmentation of continuous or edge-like deterioration patterns, while still sharing the common weaknesses on extremely sparse, point-like damage.

\subsection*{Error Modes and Observations}

Despite architectural differences, all three models exhibit systematic failure modes concentrated on a subset of damage categories. Several classes consistently obtain near-zero IoU across all architectures: \emph{Dust} and \emph{Hair} especially are never detected by any model (IoU = 0, F1 undefined). Other categories such as \emph{Burn marks}, \emph{Stamp}, \emph{Sticker}, and \emph{Puncture} are captured only by a subset of models, often with high variance across backbones: for instance, \emph{Burn marks} is segmented only by the Transformer (IoU 0.193), \emph{Stamp} only by the hybrid model (IoU 0.606), while \emph{Sticker} and \emph{Puncture} are best handled by the CNN and Transformer respectively.

Common to these difficult categories are several properties already noted in the ARTeFACT datasheet and taxonomy: they often occupy only a minute fraction of the image, present subtle edges or low contrast relative to the surrounding artwork, and are heavily imbalanced in frequency~\cite{ivanova_artefact_2024}. For \emph{Dust} and \emph{Hair}, the annotated regions amount to small, isolated blobs or thin streaks that are easily suppressed by down-sampling and feature pooling. For \emph{Stamp}, \emph{Sticker}, and \emph{Puncture}, performance is highly sensitive to the representation capacity of the encoder: some backbones learn a stable visual prototype, while others collapse to predicting \emph{Clean} everywhere.

These patterns suggest that, beyond architectural changes, further improvements will likely require re-balancing the learning problem itself. Potential directions include higher-resolution cropping around damaged areas, loss functions that explicitly up-weight rare classes, or curriculum strategies that over-sample images containing small artefacts. Another avenue is domain-specific pretraining or auxiliary objectives tailored to the morphological properties of these damages (e.g., line-like structures for \emph{Hair} and \emph{Cracks}, circular blobs for \emph{Dust}). Without such targeted interventions, current architectures—even with strong multi-scale attention—remain limited in their ability to detect extremely sparse, fine-grained deterioration.

As seen in Fig.~\ref{fig:qualitative_comparison}, the qualitative results reinforce these quantitative findings.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/dc608182-5259-46b4-b289-229fd4ad7570.jpg}
    \caption{Qualitative segmentation results on the ARTeFACT test set. From left to right: Original Image, Ground Truth, CNN (ConvNeXt), Transformer (SegFormer), and Hybrid (MaxViT). The Hybrid model demonstrates superior boundary adherence and detection of fine-grained deterioration patterns compared to the single-modality baselines.}
    \label{fig:qualitative_comparison}
\end{figure*}



\section{Discussion and Future Work}
\label{sec:Discussion}

\subsection*{Discussion}

Our experiments confirm that ARTeFACT remains a challenging benchmark, while also revealing clear differences between architecture families. All three models achieve strong performance on large, high-contrast categories such as \emph{Clean}, \emph{Material loss}, and \emph{Peel}, where IoU values exceed 0.6 and F1-scores are typically above 0.75. The hybrid model (UNet + MaxViT-T) consistently delivers the best overall performance, with the highest mean IoU (0.376) and mean F1 (0.681), as well as the best IoUs for most “core” damage types, including \emph{Clean} (0.950), \emph{Material loss} (0.819), \emph{Peel} (0.653), \emph{Cracks} (0.290), \emph{Structural defects} (0.504), \emph{Dirt} (0.257), \emph{Staining} (0.525), \emph{Discolouration} (0.508), and \emph{Scratches} (0.467). This suggests that combining convolutional inductive biases with multi-scale attention yields more stable representations for continuous or edge-like deterioration patterns.

The Transformer model (UNet + SegFormer-B3) occupies a strong middle ground: it improves substantially over the CNN baseline and is particularly competitive on some rare or structured categories, such as \emph{Burn marks} (the only model with non-zero IoU, 0.193) and \emph{Lightleak} (IoU 0.667, the best among all three). The CNN baseline (UNet + ConvNeXt-T), while clearly weaker in overall mIoU and F1, remains competitive in specific classes, achieving the best IoU and F1 for \emph{Sticker} and comparable performance on \emph{Puncture}, and it is the fastest at inference time (7.10 ms per image). Across all architectures, however, very fine and extremely sparse categories such as \emph{Dust} and \emph{Hair} remain unsolved (IoU = 0 for every model), highlighting that model capacity and attention mechanisms alone are insufficient to overcome severe class imbalance, tiny object size, and low contrast in these damage types.

\subsection*{Limitations}

This study has several limitations that qualify the interpretation of these results. First, due to time constraints, we could not perform $k$-fold cross-validation on ARTeFACT; all reported metrics come from a single stratified train/validation/test split and may be sensitive to that particular partition. Second, there is a slight mismatch in backbone capacity: SegFormer-B3 is somewhat larger than ConvNeXt-Tiny and MaxViT-Tiny, which introduces a mild bias in favor of the Transformer model when comparing architectures. Third, we did not perform explicit dataset-level class balancing beyond loss reweighting: we did not apply stratified sampling, oversampling of rare classes, or targeted augmentation focused on underrepresented damage types. As a result, the models see highly imbalanced data and fail disproportionately on rare classes; for the smallest categories, some validation or test subsets may contain very few or even zero examples, making per-class metrics unstable or undefined. Finally, our evaluation is purely in-domain: all models are trained and tested only on ARTeFACT, so we do not measure robustness under domain shift (e.g., different collections, imaging pipelines, or taxonomies), and we cannot yet claim that the observed rankings will hold on other heritage datasets.

\subsection*{Future Work}

Future work should first address data coverage and balance. Expanding ARTeFACT with additional examples for rare categories and new damage types, and constructing more balanced splits via stratified sampling and class-aware cropping, would make the benchmark more representative of real conservation practice and reduce the systematic failures observed for extremely sparse classes such as \emph{Dust} and \emph{Hair}. A second priority is to train and evaluate models on a broader range of collections and artwork types, with a particular focus on Costa Rican cultural heritage. This includes testing ARTeFACT-trained models in a zero-shot manner on local collections and studying domain adaptation or fine-tuning strategies when performance degrades. Finally, the training pipeline could be extended with stronger imbalance-aware techniques—such as targeted augmentations for rare damage, class-balanced or focal-style losses tailored to long-tailed distributions, and higher-resolution crops around annotated regions—combined with more systematic experimental protocols (e.g., cross-validation). Together, these steps would move towards more robust, transferable models that can support conservators in diverse institutional settings.

\section*{Acknowledgment}

% Escuela de Ciencias de la Computación e Informática (ECCI)
% Centro de Investigaciones en Tecnologías de la Información y Comunicación (CITIC)
This research was supported by \ifanonymous [Anonymized entities]\else the School of Computer Science and Informatics (ECCI), and the Center for Research in Information and Communication Technologies (CITIC), both of the University of Costa Rica (UCR)\fi. 

The following AI tools were used in the preparation of this article: DeepL Translate and Overleaf's AI-powered language tools for spellchecking and wording suggestions. 

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}
