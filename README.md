# HeritageArt-CNN-ViT-Hybrid

A hybrid CNN-ViT architecture for heritage artifact damage segmentation using the ARTeFACT dataset.

##  Repository Structure

```
HeritageArt-CNN-ViT-Hybrid/
â”œâ”€â”€ pipeline/                           # Main training pipeline (planned)
â”‚   â”œâ”€â”€ src/                           # Source code (models, datasets, training)
â”‚   â”œâ”€â”€ configs/                       # Configuration files
â”‚   â”œâ”€â”€ tests/                         # Unit tests
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ pyproject.toml                # Project metadata
â”‚
â”œâ”€â”€ experiments/                        # Research experiments and POCs
â”‚   â”œâ”€â”€ artefact-data-obtention/      # ARTeFACT dataset downloader
â”‚   â”‚   â””â”€â”€ data/demo/                # 50 samples (binary segmentation)
â”‚   â”œâ”€â”€ artefact-multibackbone-upernet/ # **POC-5: Multi-backbone comparison âœ…**
â”‚   â”‚   â”œâ”€â”€ scripts/                  # Training, evaluation, comparison
â”‚   â”‚   â”œâ”€â”€ configs/                  # Model configs (ConvNeXt, Swin, MaxViT)
â”‚   â”‚   â”œâ”€â”€ logs/                     # Training logs + evaluation results
â”‚   â”‚   â””â”€â”€ README.md                 # POC-5 documentation
â”‚   â””â”€â”€ utils/                         # Utility scripts (CUDA test, etc.)
â”‚
â””â”€â”€ documentation/                      # Project documentation
    â”œâ”€â”€ main.tex                       # Research paper (LaTeX)
    â””â”€â”€ From-Paper-to-Plan.md         # Implementation roadmap
```

---

## ðŸ† POC-5 Results: Multi-Backbone Comparison

**Objective**: Compare CNN, Vision Transformer, and Hybrid architectures for heritage art damage detection.

### Architecture & Methodology

All models use:
- **Decoder**: UPerNet (Unified Perceptual Parsing Network)
  - PPM (Pyramid Pooling Module) at scales [1, 2, 3, 6]
  - FPN (Feature Pyramid Network) with 256 channels
  - Purpose: Fair comparison by isolating encoder differences
- **Dataset**: ARTeFACT 50 samples (40 train / 10 val)
- **Task**: Binary segmentation (Clean vs Damage)
- **Resolution**: 512Ã—512
- **Training**: 60 epochs, batch size 4, AdamW optimizer (lr=0.0003)
- **Loss**: DiceFocalLoss (dice=0.7, focal=0.3)

### Results Summary

| ðŸ… | Model | Type | mIoU | mF1 | Accuracy | Damage IoU | Params | Best Epoch |
|---|-------|------|------|-----|----------|------------|--------|------------|
| ðŸ¥‡ | **MaxViT-Tiny** | **Hybrid** | **71.64%** | **82.13%** | **91.01%** | **53.29%** | 39.2M | 38 |
| ðŸ¥ˆ | Swin-Tiny | Transformer | 64.23% | 76.07% | 87.21% | 42.59% | 37.4M | 34 |
| ðŸ¥‰ | ConvNeXt-Tiny | CNN | 63.67% | 75.52% | 87.16% | 41.48% | 37.7M | 38 |

### Key Findings

**ðŸš€ MaxViT (Hybrid CNN+Multi-axis Attention) is the winner:**
- **+12.5% mIoU** vs ConvNeXt (CNN)
- **+11.5% mIoU** vs Swin (Transformer)
- **+28.5% Damage IoU** vs ConvNeXt
- **+25.1% Damage IoU** vs Swin

**Architecture Insights:**
1. **Hybrid > Transformer > CNN** for heritage art damage detection
2. **MaxViT** excels at detecting damage (53.29% Damage IoU)
3. All models converge around epoch 34-38
4. Small dataset (50 samples) â†’ Hybrid better leverages ImageNet pretraining

**Per-Class Performance (MaxViT):**
- **Clean**: 89.99% IoU, 94.73% F1
- **Damage**: 53.29% IoU, 69.53% F1
- **Clean Precision**: 92.29% | **Damage Precision**: 82.09%
- **Clean Recall**: 97.30% | **Damage Recall**: 60.30%

**Detailed Results**: See `experiments/artefact-multibackbone-upernet/logs/comparison/`
- Training curves, metrics comparison, per-class IoU, convergence analysis
- Confusion matrices and prediction visualizations

---

## Prerequisites

- **Python**: 3.10 (tested on 3.10.18)
- **CUDA**: 11.8+ (for GPU training)
- **GPU**: NVIDIA GPU with 8GB+ VRAM recommended (RTX 3060 Ti or better)

---

## Installation Steps

### 1. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
```

### 2. Update pip tooling

```bash
python -m pip install -U pip setuptools wheel
```

### 3. **IMPORTANT: Install PyTorch FIRST** (before requirements.txt)

**Check your CUDA version:**
```bash
nvidia-smi  # Look for "CUDA Version: X.X"
```

**Then install the matching PyTorch build:**

```bash
# For CUDA 12.8+ (RTX 40 series, A100, etc.)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu128

# For CUDA 12.6
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu126

# For CUDA 11.8 (older GPUs: RTX 30 series, V100, etc.)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu118

# For ROCm 6.3 (AMD GPUs, Linux only)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/rocm6.3

# For CPU-only (no GPU)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cpu
```

### 4. Verify PyTorch Installation

```bash
python experiments/utils/cuda-test.py
```

**Expected output:**
```
Torch: 2.7.1+cu128
torchvision: 0.22.1+cu128
CUDA available: True
Device count: 1
Device name: NVIDIA GeForce RTX 4060
```

 Verify:
- `CUDA available: True`
- Your GPU name appears correctly

### 5. Install Project Dependencies

```bash
pip install -r requirements.txt
```

### 6. Install OpenMMLab Components (via MIM)

```bash
pip install -U openmim
mim install mmengine==0.10.4
mim install mmcv==2.1.0
mim install mmsegmentation==1.2.2
```

> **Note:** MIM automatically selects the correct `mmcv` wheel for your PyTorch/CUDA version.

### 7. Install Pre-commit Hooks (for development)

```bash
pre-commit install
```

This enables automatic code formatting and linting before each commit.

---

## Download Model Checkpoints

Use `mim` to download pre-trained models and their configs:

### PSPNet + ResNet50 (ADE20K, demo purposes)

```bash
mim download mmsegmentation \
  --config pspnet_r50-d8_4xb4-80k_ade20k-512x512 \
  --dest ./experiments/mmseg_demos/_mmseg_demo
```

This downloads a lightweight model for testing MMSegmentation installation.

### Swin-Base + UPerNet (ADE20K, 512Ã—512)

```bash
mim download mmsegmentation \
  --config swin-base-patch4-window7-in22k-pre_upernet_8xb2-160k_ade20k-512x512 \
  --dest checkpoints
```

- **Pretrain**: ImageNet-22k
- **Finetune**: ADE20K (160k iterations)
- **Resolution**: 512Ã—512

### ConvNeXt-Large + UPerNet (ADE20K, 640Ã—640, AMP)

```bash
mim download mmsegmentation \
  --config convnext-large_upernet_8xb2-amp-160k_ade20k-640x640 \
  --dest checkpoints
```

- **Pretrain**: ImageNet-22k
- **Finetune**: ADE20K (160k iterations)
- **Resolution**: 640Ã—640
- **Training**: Mixed precision (AMP)

---

## Quick Start: Run Demos

### 1. Verify CUDA Setup

```bash
python experiments/utils/cuda-test.py
```

### 2. Test MMSegmentation Installation

```bash
python experiments/mmseg_demos/MMSeg-test.py
```

### 3. Run Segmentation Demo (Swin + ConvNeXt)

```bash
# Requires checkpoints downloaded above
python experiments/mmseg_demos/image_demo.py
```

This will:
1. Load Swin-Base + UPerNet model
2. Perform inference on `demo/demo.png`
3. Display results
4. Repeat with ConvNeXt-Large + UPerNet

---

## Project Structure

```
HeritageArt-CNN-ViT-Hybrid/
â”œâ”€â”€ configs/                    # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main config (orchestrator)
â”‚   â”œâ”€â”€ model/                 # Model architectures
â”‚   â”‚   â”œâ”€â”€ cnn.yaml          # ConvNeXt + UPerNet
â”‚   â”‚   â”œâ”€â”€ vit.yaml          # Swin + UPerNet
â”‚   â”‚   â””â”€â”€ hybrid.yaml       # CoaT + UPerNet
â”‚   â”œâ”€â”€ data/                  # Dataset configs
â”‚   â”‚   â””â”€â”€ artefact.yaml     # ARTeFACT dataset
â”‚   â””â”€â”€ train/                 # Training hyperparameters
â”‚       â””â”€â”€ default.yaml
â”œâ”€â”€ src/                       # Main pipeline (follows From-Paper-to-Plan.md)
â”‚   â”œâ”€â”€ datasets/              # Dataset loaders
â”‚   â”œâ”€â”€ models/                # Model definitions
â”‚   â””â”€â”€ infer/                 # Inference scripts
â”œâ”€â”€ experiments/               # Quick experiments & demos (not part of main pipeline)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ cuda-test.py      # CUDA verification
â”‚   â””â”€â”€ mmseg_demos/
â”‚       â”œâ”€â”€ MMSeg-test.py     # MMSeg installation test
â”‚       â””â”€â”€ image_demo.py     # Segmentation demo
â”œâ”€â”€ tests/                     # Unit tests for src/
â”œâ”€â”€ checkpoints/               # Downloaded model weights (gitignored)
â”œâ”€â”€ data/                      # ARTeFACT dataset (gitignored)
â”œâ”€â”€ outputs/                   # Training outputs (gitignored)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pyproject.toml            # Tool configurations (black, isort, pytest)
â”œâ”€â”€ .pre-commit-config.yaml   # Pre-commit hooks
â””â”€â”€ .flake8                   # Linting rules
```

---

## Development Workflow

1. **Activate virtual environment**:
   ```bash
   source venv/bin/activate
   ```

2. **Make code changes** in `src/`

3. **Run pre-commit checks manually** (optional):
   ```bash
   pre-commit run --all-files
   ```

4. **Commit changes**:
   ```bash
   git add .
   git commit -m "feat: your feature description"
   ```
   Pre-commit hooks will automatically:
   - Format code with Black
   - Lint with Flake8
   - Sort imports with isort
   - Fix trailing whitespace
   - Validate YAML/JSON/TOML

---

## Generate Requirements Lock File

To create a strict, machine-specific dependency snapshot:

```bash
pip freeze --exclude-editable > requirements.lock.txt
```

Use this for exact reproducibility on the same machine/environment.

---

## Troubleshooting

### PyTorch not using GPU

```bash
# Reinstall PyTorch with correct CUDA version
pip uninstall torch torchvision -y
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu128
```

### MMSegmentation import errors

```bash
# Reinstall OpenMMLab stack
pip uninstall mmengine mmcv mmsegmentation -y
pip install -U openmim
mim install mmengine==0.10.4
mim install mmcv==2.1.0
mim install mmsegmentation==1.2.2
```

### Pre-commit hooks failing

```bash
# Reinstall pre-commit
pre-commit clean
pre-commit install
pre-commit run --all-files
```

---

## License

[Add your license here]

## Citation

If you use this code, please cite:

```bibtex
[Add citation here]
```

---

## Acknowledgments

- **ARTeFACT Dataset**: [Link to dataset paper]
- **MMSegmentation**: OpenMMLab semantic segmentation toolbox
- **PyTorch**: Deep learning framework
