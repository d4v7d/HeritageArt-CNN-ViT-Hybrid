- [ ] **Input size fairness**: the Swin config uses 512×512 while ConvNeXt config is 640×640. Methods declares a single, shared pipeline; unify both to 512×512 (and keep patch-based training).
- [ ] **Swin pretraining note**: Methods says “ideally MAE/DINOv2; otherwise 1k”. We used 22k supervised. Update the Methods text to reflect IN-22k Swin init actually used, or change SWIN version
- [ ] **Hybrid (CoaT)**: still pending install/integration
- [ ] **Unify input size** to 512×512 for both configs (fairness).
- [ ] Prepare masks with IDs [0..15] (Clean + 15 damages) and 255 as ignore for borders.
- [ ] Point the configs to data/artefact and set num_classes=16, ignore_index=255.
- [ ] Loss: CE + macro Dice (0.5/0.5) in both main and auxiliary heads.
- [ ] Optim: AdamW (lr=5e-4, wd=0.01), val each epoch, keep augmentations minimal & shared.
- [ ] Train Swin & ConvNeXt → compare macro-F1/mIoU (per class + macro).
- [ ] 