---
# Training hyperparameters
epochs: 50
gradient_accumulation_steps: 1  # Simular batch m√°s grande

# Optimizer
optimizer:
  name: adamw
  lr: 0.0005  # Base learning rate
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning Rate Scheduler
scheduler:
  name: cosine  # cosine | onecycle | reduce_on_plateau
  warmup_epochs: 5
  warmup_start_lr: 1e-6
  min_lr: 1e-7  # Para cosine annealing

  # ReduceLROnPlateau specific (si lo usas)
  patience: 5
  factor: 0.2

# Early Stopping
early_stopping:
  enabled: true
  patience: 10
  monitor: val_macro_f1  # val_loss | val_miou | val_macro_f1
  mode: max  # max para F1/IoU, min para loss

# Mixed Precision (AMP)
use_amp: true  # Acelera training y reduce VRAM

# Gradient Clipping
grad_clip:
  enabled: true
  max_norm: 1.0

# Validation
val_every_n_epochs: 1
