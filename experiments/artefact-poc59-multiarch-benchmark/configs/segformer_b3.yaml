# POC-5.9: SegFormer MiT-B3 + UNet
# Pure ViT architecture with hierarchical attention

model:
  architecture: Unet
  encoder_name: mit_b3  # SegFormer Mix Transformer B3 (SMP encoder)
  encoder_weights: imagenet
  in_channels: 3
  classes: 16
  activation: null

data:
  data_dir: ../common-data/artefact_augmented
  image_size: 384  # Uniform resolution
  num_workers: 16
  use_augmented: true
  use_preload: true
  preload_to_gpu: false

training:
  batch_size: 32  # Reduced from 96 - ViT attention has O(n²) memory
  epochs: 50
  learning_rate: 0.000333  # Scaled: 0.001 × (32/96) for batch size adjustment
  weight_decay: 0.01
  mixed_precision: true
  gradient_clip: 1.0

loss:
  type: dice  # Winner: Dice + balanced weights (27.66% mIoU ConvNeXt)
  mode: multiclass
  smooth: 1.0
  class_weights_file: "class_weights_balanced.json"  # Expected: +2-3% vs ConvNeXt

optimizer:
  type: adamw
  betas: [0.9, 0.999]

scheduler:
  type: onecycle
  max_lr: 0.000333  # Matches training.learning_rate
  pct_start: 0.3
  anneal_strategy: cos

dataloader:
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: true

augmentation:
  train:
    - Resize: [384, 384]
    - HorizontalFlip: {p: 0.5}
    - VerticalFlip: {p: 0.3}
    - RandomRotate90: {p: 0.3}
    - Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
  val:
    - Resize: [384, 384]
    - Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}

logging:
  log_dir: logs
  save_best: true
  save_last: true

# Expected metrics (ConvNeXt baseline + ViT advantage):
# - mIoU: ~30% (27.66% ConvNeXt + 2-3% ViT boost)
# - Throughput: ~90-100 imgs/s
# - Training time: ~18 min (50 epochs)
# - Params: ~45M
