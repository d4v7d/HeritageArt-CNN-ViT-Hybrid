# MaxViT-Tiny + UPerNet Configuration
# Hybrid: Convolutional + Multi-axis Attention (block + grid attention)

model:
  name: "maxvit_tiny_upernet"
  encoder: "maxvit_tiny_tf_512"
  encoder_source: "timm"
  encoder_weights: "imagenet_in1k"
  decoder: "upernet"
  classes: 2

  # UPerNet settings
  upernet:
    ppm_pool_scales: [1, 2, 3, 6]
    fpn_out_channels: 256
    dropout: 0.1

  # MaxViT-specific
  maxvit:
    img_size: 512                # Native resolution support
    window_size: 7               # Local attention window
    grid_size: 8                 # Global attention grid
    drop_path_rate: 0.2          # Stochastic depth

# Override specific training settings for memory efficiency
training:
  batch_size: 2                  # Reduced for MaxViT memory requirements
  epochs: 60                     # Full convergence
  learning_rate: 0.0003          # Optimized from POC-4
  weight_decay: 0.00005

  loss:
    type: "dice_focal"
    dice_weight: 0.7             # Favors mIoU
    focal_weight: 0.3
    focal_alpha: 0.25
    focal_gamma: 2.0

  optimizer: "adamw"

  scheduler:
    type: "cosine"
    warmup_epochs: 8             # Progressive warmup
    min_lr: 0.0000005

  checkpointing:
    save_freq: 10                # Every 10 epochs
    save_best: true
    metric: "val_miou"

  mixed_precision: true          # Enable AMP to save memory