# ARTeFACT Data Obtention - HuggingFace Streaming Approach

**Code example** demonstrating how to download ARTeFACT using HuggingFace `datasets` library with streaming mode.

âš ï¸ **Note**: This approach has memory limitations with very large images.

## ï¿½ Project Structure

```
artefact-data-obtention/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ docker/                # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile         # Image definition
â”‚   â”œâ”€â”€ docker-compose.yml # Compose config
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ scripts/               # Python scripts
â”‚   â””â”€â”€ download_artefact.py  # Main download script
â”œâ”€â”€ Makefile               # Convenience commands (make help)
â”œâ”€â”€ setup.sh               # venv setup script
â”œâ”€â”€ venv/                  # Python virtual environment (optional)
â””â”€â”€ data/                  # Downloaded datasets (generated)
```

## Purpose

Provides clean, documented code showing how to integrate ARTeFACT with HuggingFace ecosystem for training pipelines.

## Approach: HuggingFace Datasets Library

âœ… **Streaming mode** - Downloads samples incrementally without loading entire dataset  
âœ… **Automatic image resizing** - Reduces memory footprint (max 512-1024px)  
âœ… **Progress tracking** - Real-time progress with tqdm  
âœ… **Rich metadata** - Exports metadata, statistics, and visualizations  
âœ… **Docker ready** - Fully containerized, no venv conflicts  
âœ… **16GB RAM** - Successfully processes 417/418 samples (99.8%)

## Quick Start

### Option 1: Docker (Recommended - Isolated)

```bash
# Build image (first time only)
make build

# Download 10 samples (quick test)
make download-small

# Download 50 samples
make download-medium

# Download full dataset (417/418 samples, requires 16GB RAM)
make download-full

# Or use docker-compose directly:
cd docker
docker-compose run --rm artefact-data-obtention python3 scripts/download_artefact.py \
  --output data/test \
  --max-samples 10
```

### Option 2: Python venv

#### 1. Setup Environment (First Time Only)

```bash
./setup.sh
```

This creates a venv and installs all dependencies.

#### 2. Activate Environment

```bash
source venv/bin/activate
```

#### 3. Download Samples

```bash
# Download 10 samples (quick test)
python scripts/download_artefact.py --max-samples 10 --output ./data/test

# Download 50 samples (recommended for POC)
python scripts/download_artefact.py --max-samples 50 --output ./data/poc

# Download 100+ samples (for real training)
python scripts/download_artefact.py --max-samples 100 --output ./data/train

# Download ALL samples (~420 images, requires 16GB RAM)
python scripts/download_artefact.py --all --output ./data/full
```
python download_artefact.py --max-samples 100 --output ./data/artefact_train

# Download ALL samples (~420 images, requires 16GB RAM)
python download_artefact.py --all --output ./data/artefact_full
```

## Files

```
artefact-data-obtention/
â”œâ”€â”€ download_artefact.py      # Main script with streaming mode
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.sh                 # Setup script (creates venv)
â”œâ”€â”€ .gitignore              # Git ignore rules
â””â”€â”€ data/
    â””â”€â”€ artefact_venv_test/  # Sample output (5 successful downloads)
```

### download_artefact.py

**Features:**
- âœ… Streaming download (incremental processing)
- âœ… Automatic image resizing (max 512px)
- âœ… Progress tracking with tqdm
- âœ… Metadata and statistics export
- âœ… Sample visualizations (4-panel)

**Limitations:**
- âš ï¸ Crashes on extremely large images (>50M pixels)
- âš ï¸ Successfully processes ~5-9 samples before hitting memory limits
- âš ï¸ Not suitable for full dataset download from scratch

## Output Structure

```
data/artefact_*/
â”œâ”€â”€ images/              # Resized original images (PNG)
â”œâ”€â”€ annotations/         # Grayscale damage masks (0-15 classes, 255=background)
â”œâ”€â”€ annotations_rgb/     # Colored damage visualizations
â”œâ”€â”€ visualizations/      # 4-panel sample visualizations (first 10)
â”œâ”€â”€ metadata.csv         # Sample IDs, paths, descriptions, sizes
â””â”€â”€ statistics.json      # Dataset statistics (materials, contents, class distribution)
```

## Example: Sample Dataset

The `data/artefact_venv_test/` directory contains a successful download of 5 samples demonstrating the output structure.

## Known Limitations

### Memory Constraints with Large Images

**Problem:** The HuggingFace `datasets` library decodes images fully in memory when accessing `sample['image']`, BEFORE any resizing can occur. The ARTeFACT dataset contains extremely large images (up to 7680Ã—4877 = 37M pixels, some even >100M pixels) that can exceed available system memory.

**What happens:**
- âœ… Small-medium images (< 20M pixels): Process successfully
- âš ï¸ Large images (20-50M pixels): May work depending on available RAM
- âŒ Huge images (> 50M pixels): Crash WSL/system (OOM)

**Solutions attempted:**
1. âœ… Streaming mode (`streaming=True`) - Still decodes individual images fully
2. âœ… PIL thumbnail/resize - Called AFTER image already loaded in memory
3. âœ… Explicit memory cleanup - Too late, memory already consumed
4. âœ… Running outside Docker - Same fundamental issue
5. âŒ PIL draft mode - Not supported by datasets library's image loader

**Current status:** 
- **Suitable for pipeline integration** where data is pre-processed
- **Successfully downloads 5-9 samples** before hitting problematic large images
- **NOT suitable for full dataset download** from scratch

**Recommendation:** 
Use `../artefact-repo-analysis/` (git-lfs + parquet processing) for full dataset obtention. This approach gives you:
- âœ… Full control over image decoding
- âœ… Can check size BEFORE loading
- âœ… Memory-efficient processing
- âœ… Successfully processed 5 samples without issues

**Use this approach ONLY IF:**
- Data is already pre-processed and available
- You need HuggingFace ecosystem integration
- Working with subset of smaller images
- For pipeline training code examples

## âš ï¸ For Production Use

**This approach is for learning/examples only.** For actual dataset obtention:

ğŸ‘‰ **Use [`../artefact-repo-analysis/`](../artefact-repo-analysis/)** 
- âœ… Handles all images including 133M pixel files
- âœ… Memory efficient
- âœ… Production-ready
- âœ… Successfully processed 15/15 samples from first parquet file

## Dataset Structure

Expected output:
```
data/artefact_custom/
â”œâ”€â”€ images/              # Original images (resized)
â”œâ”€â”€ annotations/         # Grayscale damage masks
â”œâ”€â”€ annotations_rgb/     # Colored damage visualizations
â”œâ”€â”€ visualizations/      # Sample 4-panel visualizations
â”œâ”€â”€ metadata.csv         # Sample metadata
â””â”€â”€ statistics.json      # Dataset statistics
```

## Related Experiments

- **[`../artefact-repo-analysis/`](../artefact-repo-analysis/)** - Production data obtention (git-lfs + parquet)
- **[`../poc-art-damage/`](../poc-art-damage/)** - Main training pipeline with CNN/ViT models
- **[`../ARTEFACT_EXPERIMENTS.md`](../ARTEFACT_EXPERIMENTS.md)** - Overview of both approaches
