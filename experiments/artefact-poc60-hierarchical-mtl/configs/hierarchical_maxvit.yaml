# POC-60: MaxViT-Tiny + Hierarchical UPerNet
# Innovation #1: Hierarchical Multi-Task Learning (Binary + Coarse + Fine)

model:
  architecture: HierarchicalUPerNet
  encoder: maxvit_tiny_tf_384
  encoder_name: maxvit_tiny_tf_384    # For compatibility
  encoder_weights: imagenet_in1k
  classes: 16
  activation: null
  hierarchical: true  # Enable hierarchical mode
  
  # UPerNet decoder params
  upernet:
    ppm_pool_scales: [1, 2, 3, 6]
    fpn_out_channels: 256
    dropout: 0.1

data:
  data_dir: ../common-data/artefact_augmented
  image_size: 384
  num_workers: 16
  use_augmented: true
  use_preload: true
  preload_to_gpu: false

training:
  batch_size: 32  # Conservative for hierarchical model
  epochs: 100  # No curriculum (POC-60), regular training
  learning_rate: 0.0001
  weight_decay: 0.01
  mixed_precision: true
  gradient_clip: 1.0

loss:
  type: hierarchical  # Special hierarchical loss
  mode: multiclass
  smooth: 1.0
  # Loss weights: 0.2 * binary + 0.3 * coarse + 1.0 * fine
  binary_weight: 0.2
  coarse_weight: 0.3
  fine_weight: 1.0

optimizer:
  type: adamw
  betas: [0.9, 0.999]

scheduler:
  type: onecycle
  max_lr: 0.0001
  pct_start: 0.3
  anneal_strategy: cos

dataloader:
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: true

augmentation:
  train:
    - Resize: [384, 384]
    - HorizontalFlip: {p: 0.5}
    - VerticalFlip: {p: 0.3}
    - RandomRotate90: {p: 0.3}
    - Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
  val:
    - Resize: [384, 384]
    - Normalize: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}

logging:
  log_dir: logs
  save_best: true
  save_last: true

# Expected metrics (POC-60 target):
# - mIoU Binary: 70-74%
# - mIoU Coarse: 55-59%
# - mIoU Fine: 42-45% (+7-11% vs POC-59 baseline 34.58%)
# - Training time: ~1.4h @ 100 epochs (V100 32GB)
